{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6dcbed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-SSMFL1C:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TransportAnalytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1740ba33cd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Function to create a Spark session\n",
    "def get_spark_session(app_name=\"TransportAnalytics\"):\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Start the session\n",
    "spark = get_spark_session()\n",
    "\n",
    "#Check if it worked\n",
    "print(\"Spark Session Created!\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c40cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 18072024142824.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Centaur Coaches_421\n",
      "Extracting: Clarkes_of_London_2023-11-18_13-22_1.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Clarkes of London_422\n",
      "Extracting: 18947_106818_2026-01-08_16-01-56.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Edward Thomas_326\n",
      "Extracting: 17058_108571_2026-02-04_16-02-25_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Ensign Bus Co Ltd_18\n",
      "Extracting: 21332_108435_2026-02-02_16-01-39_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Falcon Buses_237\n",
      "Extracting: current_CqGEiDo.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\First Bus London_238\n",
      "Extracting: 2282_107652_2026-01-20_16-05-37_RRAR.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\FirstGroup plc_16\n",
      "Extracting: 2648_108412_2026-02-02_04-01-50_FTVA.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\FirstGroup plc_16\n",
      "Extracting: 13649_108260_2026-01-30_04-02-56_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Go-Ahead Group plc_10\n",
      "Extracting: 15884_108442_2026-02-02_16-05-02_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Go-Ahead Group plc_10\n",
      "Extracting: 17163_108632_2026-02-05_12-48-43_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Go-Ahead Group plc_10\n",
      "Extracting: 21433_106910_2026-01-09_16-02-02_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\GO-Coach Hire Ltd_329\n",
      "Extracting: Golden_Tours_Sightseeing_Ltd_2023-04-21.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Golden Tours_592\n",
      "Extracting: Metroline_TfL_9w6A35W.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Metroline Travel Limited_24\n",
      "Extracting: NIBS.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\NIBSbuses_67\n",
      "Extracting: 21305_108573_2026-02-04_16-03-33_TXC_16_10001.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Reading Buses_7\n",
      "Extracting: 21306_108165_2026-01-28_16-02-46_TXC_16_10003.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Reading Buses_7\n",
      "Extracting: 11526_107902_2026-01-24_04-02-47_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Red Eagle_255\n",
      "Extracting: 19965_108100_2026-01-27_16-01-55_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Red Rose Travel_251\n",
      "Extracting: 12635_107836_2026-01-23_04-07-06_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Reptons Coaches_240\n",
      "Extracting: 12447_105880_2025-12-29_04-15-21_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Rotala Group of Companies_53\n",
      "Extracting: 18064_108616_2026-02-05_04-04-05_stagecoach-scox-route-schedule-data-transxchange_2_4.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Stagecoach Group_15\n",
      "Extracting: SL_180925.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Stagecoach Group_15\n",
      "Extracting: 5296_107705_2026-01-21_16-01-50_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Sullivan Buses_244\n",
      "Extracting: 21059_106407_2026-01-04_16-02-43_TXC_16_10002.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Thames Valley Buses_234\n",
      "Extracting: 10405_108436_2026-02-02_16-01-38_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\Uno_22\n",
      "Extracting: 21598_106463_2026-01-05_13-24-13_current.zip in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS\\White Bus_246\n",
      "--- Extraction Finished ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Set your base path\n",
    "base_path = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\bodds_archive_20260206_Hi4S9OS'\n",
    "\n",
    "def extract_nested_zips(root_directory):\n",
    "    # Walk through the entire directory tree\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.zip'):\n",
    "                zip_path = os.path.join(root, file)\n",
    "                # Define where to extract (extracting into the current 'root' folder)\n",
    "                extract_path = root \n",
    "                \n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                        print(f\"Extracting: {file} in {root}\")\n",
    "                        zip_ref.extractall(extract_path)\n",
    "                    \n",
    "                    # Optional: Remove the zip file after extracting to save space\n",
    "                    # os.remove(zip_path) \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to unzip {file}: {e}\")\n",
    "\n",
    "# Run the function\n",
    "extract_nested_zips(base_path)\n",
    "print(\"--- Extraction Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99001b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 177 XML files in Ensign Bus folder\n",
      "Sample files: ['ENSB_22E_ENSBPF00019613722_20260105_20260211_2235426.xml', 'ENSB_22E_ENSBPF00019613722_20260108_20260212_2235427.xml', 'ENSB_22E_ENSBPF00019613722_20260109_20260213_2235428.xml', 'ENSB_22E_ENSBPF00019613722_20260216_20260220_2295873.xml', 'ENSB_22E_ENSBPF00019613722_20260223_20260325_2295895.xml']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Parse Ensign Bus TransXChange Timetable XMLs\n",
    "# ============================================================\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "# TransXChange namespace\n",
    "NS = {'txc': 'http://www.transxchange.org.uk/'}\n",
    "\n",
    "ensign_path = os.path.join(base_path, 'Ensign Bus Co Ltd_18')\n",
    "xml_files = [f for f in os.listdir(ensign_path) if f.endswith('.xml')]\n",
    "print(f\"Found {len(xml_files)} XML files in Ensign Bus folder\")\n",
    "print(f\"Sample files: {xml_files[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591e4bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Helper functions to parse TransXChange XML\n",
    "# ============================================================\n",
    "\n",
    "def parse_iso_duration(duration_str):\n",
    "    \"\"\"Convert ISO 8601 duration (e.g., PT2M, PT1H30M) to total minutes.\"\"\"\n",
    "    if not duration_str:\n",
    "        return 0\n",
    "    match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', duration_str)\n",
    "    if not match:\n",
    "        return 0\n",
    "    hours = int(match.group(1) or 0)\n",
    "    minutes = int(match.group(2) or 0)\n",
    "    seconds = int(match.group(3) or 0)\n",
    "    return hours * 60 + minutes + seconds / 60\n",
    "\n",
    "def parse_transxchange(filepath):\n",
    "    \"\"\"Parse a single TransXChange XML file and return structured data.\"\"\"\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    # --- 1. Stop Points ---\n",
    "    stops = {}\n",
    "    for sp in root.findall('.//txc:StopPoints/txc:AnnotatedStopPointRef', NS):\n",
    "        ref = sp.findtext('txc:StopPointRef', '', NS)\n",
    "        stops[ref] = {\n",
    "            'stop_ref': ref,\n",
    "            'stop_name': sp.findtext('txc:CommonName', '', NS).strip(),\n",
    "            'longitude': float(sp.findtext('txc:Location/txc:Longitude', '0', NS)),\n",
    "            'latitude': float(sp.findtext('txc:Location/txc:Latitude', '0', NS)),\n",
    "        }\n",
    "    \n",
    "    # --- 2. Operators ---\n",
    "    operators = {}\n",
    "    for op in root.findall('.//txc:Operators/txc:Operator', NS):\n",
    "        op_id = op.get('id')\n",
    "        operators[op_id] = {\n",
    "            'national_operator_code': op.findtext('txc:NationalOperatorCode', '', NS),\n",
    "            'operator_short_name': op.findtext('txc:OperatorShortName', '', NS),\n",
    "        }\n",
    "    \n",
    "    # --- 3. Services (Lines, Operating Period, Days of Week) ---\n",
    "    services = {}\n",
    "    journey_patterns = {}\n",
    "    for svc in root.findall('.//txc:Services/txc:Service', NS):\n",
    "        svc_code = svc.findtext('txc:ServiceCode', '', NS)\n",
    "        \n",
    "        # Lines\n",
    "        line_el = svc.find('txc:Lines/txc:Line', NS)\n",
    "        line_name = line_el.findtext('txc:LineName', '', NS) if line_el is not None else ''\n",
    "        line_id = line_el.get('id', '') if line_el is not None else ''\n",
    "        \n",
    "        # Operating Period\n",
    "        start_date = svc.findtext('txc:OperatingPeriod/txc:StartDate', '', NS)\n",
    "        end_date = svc.findtext('txc:OperatingPeriod/txc:EndDate', '', NS)\n",
    "        \n",
    "        # Days of Week\n",
    "        days_el = svc.find('.//txc:OperatingProfile/txc:RegularDayType/txc:DaysOfWeek', NS)\n",
    "        days_of_week = []\n",
    "        if days_el is not None:\n",
    "            for day in days_el:\n",
    "                day_tag = day.tag.replace('{http://www.transxchange.org.uk/}', '')\n",
    "                days_of_week.append(day_tag)\n",
    "        \n",
    "        # Origin/Destination\n",
    "        origin = svc.findtext('.//txc:StandardService/txc:Origin', '', NS)\n",
    "        destination = svc.findtext('.//txc:StandardService/txc:Destination', '', NS)\n",
    "        op_ref = svc.findtext('txc:RegisteredOperatorRef', '', NS)\n",
    "        \n",
    "        services[svc_code] = {\n",
    "            'service_code': svc_code,\n",
    "            'line_name': line_name,\n",
    "            'line_id': line_id,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'days_of_week': ', '.join(days_of_week),\n",
    "            'origin': origin,\n",
    "            'destination': destination,\n",
    "            'operator_ref': op_ref,\n",
    "        }\n",
    "        \n",
    "        # Journey Patterns (direction, destination display, section ref)\n",
    "        for jp in svc.findall('.//txc:StandardService/txc:JourneyPattern', NS):\n",
    "            jp_id = jp.get('id')\n",
    "            journey_patterns[jp_id] = {\n",
    "                'direction': jp.findtext('txc:Direction', '', NS),\n",
    "                'destination_display': jp.findtext('txc:DestinationDisplay', '', NS),\n",
    "                'journey_pattern_section_ref': jp.findtext('txc:JourneyPatternSectionRefs', '', NS),\n",
    "            }\n",
    "    \n",
    "    # --- 4. JourneyPatternSections (stop sequences + timings) ---\n",
    "    jp_sections = {}\n",
    "    for jps in root.findall('.//txc:JourneyPatternSections/txc:JourneyPatternSection', NS):\n",
    "        jps_id = jps.get('id')\n",
    "        timing_links = []\n",
    "        for tl in jps.findall('txc:JourneyPatternTimingLink', NS):\n",
    "            from_el = tl.find('txc:From', NS)\n",
    "            to_el = tl.find('txc:To', NS)\n",
    "            timing_links.append({\n",
    "                'from_seq': int(from_el.get('SequenceNumber', 0)) if from_el is not None else 0,\n",
    "                'from_stop': from_el.findtext('txc:StopPointRef', '', NS) if from_el is not None else '',\n",
    "                'from_activity': from_el.findtext('txc:Activity', 'pickUpAndSetDown', NS) if from_el is not None else '',\n",
    "                'from_wait_time': parse_iso_duration(from_el.findtext('txc:WaitTime', '', NS)) if from_el is not None else 0,\n",
    "                'to_seq': int(to_el.get('SequenceNumber', 0)) if to_el is not None else 0,\n",
    "                'to_stop': to_el.findtext('txc:StopPointRef', '', NS) if to_el is not None else '',\n",
    "                'run_time': parse_iso_duration(tl.findtext('txc:RunTime', '', NS)),\n",
    "            })\n",
    "        jp_sections[jps_id] = timing_links\n",
    "    \n",
    "    # --- 5. Vehicle Journeys ---\n",
    "    journeys = []\n",
    "    for vj in root.findall('.//txc:VehicleJourneys/txc:VehicleJourney', NS):\n",
    "        vj_code = vj.findtext('txc:VehicleJourneyCode', '', NS)\n",
    "        dep_time = vj.findtext('txc:DepartureTime', '', NS)\n",
    "        svc_ref = vj.findtext('txc:ServiceRef', '', NS)\n",
    "        line_ref = vj.findtext('txc:LineRef', '', NS)\n",
    "        jp_ref = vj.findtext('txc:JourneyPatternRef', '', NS)\n",
    "        op_ref = vj.findtext('txc:OperatorRef', '', NS)\n",
    "        journey_code = vj.findtext('.//txc:Operational/txc:TicketMachine/txc:JourneyCode', '', NS)\n",
    "        \n",
    "        # Resolve references\n",
    "        svc_info = services.get(svc_ref, {})\n",
    "        op_info = operators.get(op_ref, {})\n",
    "        jp_info = journey_patterns.get(jp_ref, {})\n",
    "        \n",
    "        journeys.append({\n",
    "            'file_name': filename,\n",
    "            'vehicle_journey_code': vj_code,\n",
    "            'journey_code': journey_code,\n",
    "            'departure_time': dep_time,\n",
    "            'service_code': svc_ref,\n",
    "            'line_name': svc_info.get('line_name', ''),\n",
    "            'direction': jp_info.get('direction', ''),\n",
    "            'destination_display': jp_info.get('destination_display', ''),\n",
    "            'origin': svc_info.get('origin', ''),\n",
    "            'destination': svc_info.get('destination', ''),\n",
    "            'start_date': svc_info.get('start_date', ''),\n",
    "            'end_date': svc_info.get('end_date', ''),\n",
    "            'days_of_week': svc_info.get('days_of_week', ''),\n",
    "            'operator_code': op_info.get('national_operator_code', ''),\n",
    "            'operator_name': op_info.get('operator_short_name', ''),\n",
    "            'journey_pattern_ref': jp_ref,\n",
    "            'journey_pattern_section_ref': jp_info.get('journey_pattern_section_ref', ''),\n",
    "        })\n",
    "    \n",
    "    return stops, journeys, jp_sections, journey_patterns\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ce5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260105_20260211_2235426.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260108_20260212_2235427.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260109_20260213_2235428.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260216_20260220_2295873.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260223_20260325_2295895.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260226_20260326_2295896.xml -> 2 journeys\n",
      "  Parsed: ENSB_22E_ENSBPF00019613722_20260227_20260327_2295897.xml -> 2 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260105_20260211_2235429.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260108_20260212_2235430.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260109_20260213_2235431.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260216_20260220_2295876.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260223_20260325_2295899.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260226_20260326_2295900.xml -> 21 journeys\n",
      "  Parsed: ENSB_22P_ENSBPF00019613722_20260227_20260327_2295901.xml -> 21 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20251227_20260321_2235291.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260105_20260211_2235434.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260108_20260212_2235435.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260109_20260213_2253751.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260216_20260220_2295874.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260223_20260325_2295902.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260226_20260326_2295903.xml -> 14 journeys\n",
      "  Parsed: ENSB_22T_ENSBPF00019613722_20260227_20260327_2295904.xml -> 14 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20251227_20260321_2235292.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20251228_20260322_2235341.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260105_20260211_2253758.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260108_20260212_2253759.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260109_20260213_2253760.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260216_20260220_2295875.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260223_20260325_2295907.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260226_20260326_2295908.xml -> 4 journeys\n",
      "  Parsed: ENSB_22Z_ENSBPF00019613722_20260227_20260327_2295909.xml -> 4 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20251227_20260321_2235290.xml -> 55 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20251228_20260322_2235340.xml -> 26 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260105_20260211_2235418.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260105_20260211_2235419.xml -> 1 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260108_20260212_2235420.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260108_20260212_2235421.xml -> 1 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260109_20260213_2235422.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260109_20260213_2235423.xml -> 1 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260216_20260220_2295877.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260223_20260325_2295885.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260223_20260325_2295886.xml -> 1 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260226_20260326_2295887.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260226_20260326_2295888.xml -> 1 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260227_20260327_2295889.xml -> 79 journeys\n",
      "  Parsed: ENSB_22_ENSBPF00019613722_20260227_20260327_2295890.xml -> 1 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20251227_20260321_2235293.xml -> 19 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260105_20260211_2253765.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260105_20260211_2253766.xml -> 6 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260105_20260211_2253767.xml -> 1 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260108_20260212_2253768.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260108_20260212_2253769.xml -> 6 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260108_20260212_2253770.xml -> 1 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260109_20260206_2253771.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260109_20260206_2253772.xml -> 6 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260213_20260213_2295911.xml -> 41 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260216_20260220_2295863.xml -> 41 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260223_20260325_2295912.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260223_20260325_2295913.xml -> 6 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260223_20260325_2295914.xml -> 1 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260226_20260326_2295915.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260226_20260326_2295916.xml -> 6 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260226_20260326_2295917.xml -> 1 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260227_20260327_2295918.xml -> 35 journeys\n",
      "  Parsed: ENSB_33_ENSBPF00019613433_20260227_20260327_2295919.xml -> 6 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20251227_20260321_2235294.xml -> 53 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20251228_20260322_2235342.xml -> 24 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260105_20260211_2253782.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260106_20260211_2253783.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260106_20260211_2253784.xml -> 2 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260108_20260212_2253785.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260108_20260212_2253786.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260108_20260212_2253787.xml -> 2 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260109_20260213_2253788.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260109_20260213_2253789.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260109_20260213_2253790.xml -> 2 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260216_20260220_2295861.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260223_20260325_2295950.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260223_20260325_2295951.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260223_20260325_2295952.xml -> 2 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260226_20260326_2295953.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260226_20260326_2295954.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260226_20260326_2295955.xml -> 2 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260227_20260327_2295956.xml -> 66 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260227_20260327_2295957.xml -> 1 journeys\n",
      "  Parsed: ENSB_44_ENSBPF00019613144_20260227_20260327_2295958.xml -> 2 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20251227_20260321_2235296.xml -> 5 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20251228_20260322_2235343.xml -> 4 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260105_20260211_2253969.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260108_20260212_2253970.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260109_20260213_2253971.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260216_20260220_2295862.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260223_20260325_2295987.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260226_20260326_2295988.xml -> 16 journeys\n",
      "  Parsed: ENSB_73A_ENSBPF00019613273_20260227_20260327_2295989.xml -> 16 journeys\n",
      "  Parsed: ENSB_73C_ENSBPF00019613273_20251228_20260322_2235344.xml -> 59 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20251227_20260321_2235297.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260105_20260211_2253986.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260108_20260212_2253987.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260109_20260213_2253988.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260216_20260220_2295867.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260223_20260325_2295998.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260226_20260326_2295999.xml -> 20 journeys\n",
      "  Parsed: ENSB_73E_ENSBPF00019613273_20260227_20260327_2296000.xml -> 20 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20251227_20260321_2235298.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20251228_20260322_2235345.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260105_20260211_2253989.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260108_20260212_2253990.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260109_20260213_2253991.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260216_20260220_2295855.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260223_20260325_2296001.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260226_20260326_2296002.xml -> 4 journeys\n",
      "  Parsed: ENSB_73Z_ENSBPF00019613273_20260227_20260327_2296003.xml -> 4 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20251227_20260321_2235295.xml -> 45 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260105_20260211_2253960.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260105_20260211_2253961.xml -> 2 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260105_20260211_2253962.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260108_20260212_2253963.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260108_20260212_2253964.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260108_20260212_2253965.xml -> 2 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260108_20260212_2253966.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260109_20260213_2253967.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260216_20260220_2295859.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260223_20260325_2295978.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260223_20260325_2295979.xml -> 2 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260223_20260325_2295980.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260226_20260326_2295981.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260226_20260326_2295982.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260226_20260326_2295983.xml -> 2 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260226_20260326_2295984.xml -> 1 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260227_20260327_2295985.xml -> 52 journeys\n",
      "  Parsed: ENSB_73_ENSBPF00019613273_20260227_20260327_2295986.xml -> 2 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20251227_20260321_2235334.xml -> 48 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260105_20260211_2253995.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260108_20260212_2253996.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260109_20260213_2253997.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260216_20260220_2295866.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260223_20260325_2296004.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260226_20260326_2296005.xml -> 61 journeys\n",
      "  Parsed: ENSB_83_ENSBPF00019612183_20260227_20260327_2296006.xml -> 61 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20250104_20250419_2003137.xml -> 18 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20250407_20250417_2065571.xml -> 28 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260118_20260322_2258390.xml -> 18 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260119_20260213_2258391.xml -> 27 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260119_20260213_2258392.xml -> 1 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260124_20260321_2258393.xml -> 18 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260216_20260220_2295865.xml -> 28 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260223_20260327_2296023.xml -> 27 journeys\n",
      "  Parsed: ENSB_88_ENSBPF00019616588_20260223_20260327_2296024.xml -> 1 journeys\n",
      "  Parsed: ENSB_99OT_ENSBPF000196110999OT_20250524_20250914_2126829.xml -> 18 journeys\n",
      "  Parsed: ENSB_99OT_ENSBPF000196110999OT_20250719_20250903_2156630.xml -> 36 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20251227_20260321_2235337.xml -> 29 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260105_20260211_2254004.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260108_20260212_2254005.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260109_20260213_2254006.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260216_20260220_2295854.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260223_20260325_2296025.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260226_20260326_2296026.xml -> 30 journeys\n",
      "  Parsed: ENSB_99_ENSBPF00019615799_20260227_20260327_2296027.xml -> 30 journeys\n",
      "  Parsed: ENSB_x1_ENSBPF000196168x1_20251226_20251226_2235278.xml -> 22 journeys\n",
      "  Parsed: ENSB_x2_ENSBPF0001961110x2_20251226_20251226_2235279.xml -> 46 journeys\n",
      "  Parsed: ENSB_x32_ENSBPF0001961103x32_20251108_20251108_2201388.xml -> 58 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20251227_20260321_2235338.xml -> 28 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20251228_20260322_2235346.xml -> 16 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260105_20260211_2254017.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260105_20260211_2254020.xml -> 1 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260108_20260212_2254021.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260108_20260212_2254022.xml -> 1 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260109_20260213_2254024.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260109_20260213_2254025.xml -> 1 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260216_20260220_2295868.xml -> 32 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260223_20260325_2296028.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260223_20260325_2296029.xml -> 1 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260226_20260326_2296030.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260226_20260326_2296031.xml -> 1 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260227_20260327_2296032.xml -> 31 journeys\n",
      "  Parsed: ENSB_x80_ENSBPF000196120x80_20260227_20260327_2296033.xml -> 1 journeys\n",
      "\n",
      "--- Parsing Complete ---\n",
      "Total unique stops: 505\n",
      "Total vehicle journeys: 3986\n",
      "Total journey pattern sections: 25\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Parse ALL Ensign Bus XML files & build DataFrames\n",
    "# ============================================================\n",
    "\n",
    "all_stops = {}\n",
    "all_journeys = []\n",
    "all_jp_sections = {}\n",
    "all_journey_patterns = {}\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    filepath = os.path.join(ensign_path, xml_file)\n",
    "    try:\n",
    "        stops, journeys, jp_sections, journey_patterns = parse_transxchange(filepath)\n",
    "        all_stops.update(stops)\n",
    "        all_journeys.extend(journeys)\n",
    "        all_jp_sections.update(jp_sections)\n",
    "        all_journey_patterns.update(journey_patterns)\n",
    "        print(f\"  Parsed: {xml_file} -> {len(journeys)} journeys\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {xml_file} -> {e}\")\n",
    "\n",
    "print(f\"\\n--- Parsing Complete ---\")\n",
    "print(f\"Total unique stops: {len(all_stops)}\")\n",
    "print(f\"Total vehicle journeys: {len(all_journeys)}\")\n",
    "print(f\"Total journey pattern sections: {len(all_jp_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3c042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timetable DataFrame shape: (34922, 22)\n",
      "\n",
      "Columns: ['file_name', 'vehicle_journey_code', 'journey_code', 'departure_time', 'service_code', 'line_name', 'direction', 'origin', 'destination', 'start_date', 'end_date', 'days_of_week', 'operator_code', 'operator_name', 'stop_sequence', 'stop_ref', 'stop_name', 'longitude', 'latitude', 'scheduled_arrival', 'wait_time_min', 'run_time_min']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>vehicle_journey_code</th>\n",
       "      <th>journey_code</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>service_code</th>\n",
       "      <th>line_name</th>\n",
       "      <th>direction</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>operator_code</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_ref</th>\n",
       "      <th>stop_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>scheduled_arrival</th>\n",
       "      <th>wait_time_min</th>\n",
       "      <th>run_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>1</td>\n",
       "      <td>1590014001</td>\n",
       "      <td>Chafford Hundred Station</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>51.485761</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>2</td>\n",
       "      <td>1590050901</td>\n",
       "      <td>Fleming Road</td>\n",
       "      <td>0.288737</td>\n",
       "      <td>51.487744</td>\n",
       "      <td>17:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>3</td>\n",
       "      <td>1590060113</td>\n",
       "      <td>Lakeside Bus Station, N</td>\n",
       "      <td>0.282997</td>\n",
       "      <td>51.490458</td>\n",
       "      <td>17:48:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>4</td>\n",
       "      <td>1590012401</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>0.273749</td>\n",
       "      <td>51.487820</td>\n",
       "      <td>17:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>5</td>\n",
       "      <td>2400101304</td>\n",
       "      <td>Galleon Boulevard</td>\n",
       "      <td>0.256627</td>\n",
       "      <td>51.455895</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>6</td>\n",
       "      <td>2400A009090A</td>\n",
       "      <td>Stone Crossing</td>\n",
       "      <td>0.265888</td>\n",
       "      <td>51.452233</td>\n",
       "      <td>18:01:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>7</td>\n",
       "      <td>2400A060760R</td>\n",
       "      <td>Asda</td>\n",
       "      <td>0.277592</td>\n",
       "      <td>51.451388</td>\n",
       "      <td>18:03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>8</td>\n",
       "      <td>240098192</td>\n",
       "      <td>Greenhithe Station</td>\n",
       "      <td>0.280411</td>\n",
       "      <td>51.450767</td>\n",
       "      <td>18:03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>9</td>\n",
       "      <td>2400107374</td>\n",
       "      <td>St Clements Lakes</td>\n",
       "      <td>0.281866</td>\n",
       "      <td>51.445322</td>\n",
       "      <td>18:04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>ENSB</td>\n",
       "      <td>Ensignbus</td>\n",
       "      <td>10</td>\n",
       "      <td>2400A070110A</td>\n",
       "      <td>Bluewater Bus Station</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>51.437720</td>\n",
       "      <td>18:08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name vehicle_journey_code  \\\n",
       "0  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "1  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "2  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "3  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "4  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "5  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "6  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "7  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "8  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "9  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "\n",
       "  journey_code departure_time  service_code line_name direction  \\\n",
       "0           97       17:43:00  PF0001961:37        22  outbound   \n",
       "1           97       17:43:00  PF0001961:37        22  outbound   \n",
       "2           97       17:43:00  PF0001961:37        22  outbound   \n",
       "3           97       17:43:00  PF0001961:37        22  outbound   \n",
       "4           97       17:43:00  PF0001961:37        22  outbound   \n",
       "5           97       17:43:00  PF0001961:37        22  outbound   \n",
       "6           97       17:43:00  PF0001961:37        22  outbound   \n",
       "7           97       17:43:00  PF0001961:37        22  outbound   \n",
       "8           97       17:43:00  PF0001961:37        22  outbound   \n",
       "9           97       17:43:00  PF0001961:37        22  outbound   \n",
       "\n",
       "                                     origin destination  start_date  ...  \\\n",
       "0  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "1  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "2  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "3  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "4  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "5  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "6  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "7  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "8  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "9  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "\n",
       "  operator_code operator_name stop_sequence      stop_ref  \\\n",
       "0          ENSB     Ensignbus             1    1590014001   \n",
       "1          ENSB     Ensignbus             2    1590050901   \n",
       "2          ENSB     Ensignbus             3    1590060113   \n",
       "3          ENSB     Ensignbus             4    1590012401   \n",
       "4          ENSB     Ensignbus             5    2400101304   \n",
       "5          ENSB     Ensignbus             6  2400A009090A   \n",
       "6          ENSB     Ensignbus             7  2400A060760R   \n",
       "7          ENSB     Ensignbus             8     240098192   \n",
       "8          ENSB     Ensignbus             9    2400107374   \n",
       "9          ENSB     Ensignbus            10  2400A070110A   \n",
       "\n",
       "                  stop_name longitude   latitude  scheduled_arrival  \\\n",
       "0  Chafford Hundred Station  0.287746  51.485761           17:43:00   \n",
       "1              Fleming Road  0.288737  51.487744           17:44:00   \n",
       "2   Lakeside Bus Station, N  0.282997  51.490458           17:48:00   \n",
       "3                     Tesco  0.273749  51.487820           17:50:00   \n",
       "4         Galleon Boulevard  0.256627  51.455895           18:00:00   \n",
       "5            Stone Crossing  0.265888  51.452233           18:01:00   \n",
       "6                      Asda  0.277592  51.451388           18:03:00   \n",
       "7        Greenhithe Station  0.280411  51.450767           18:03:00   \n",
       "8         St Clements Lakes  0.281866  51.445322           18:04:00   \n",
       "9     Bluewater Bus Station  0.275320  51.437720           18:08:00   \n",
       "\n",
       "   wait_time_min run_time_min  \n",
       "0              0          1.0  \n",
       "1              0          1.0  \n",
       "2              0          4.0  \n",
       "3              0          2.0  \n",
       "4              0         10.0  \n",
       "5              0          1.0  \n",
       "6              0          2.0  \n",
       "7              0          0.0  \n",
       "8              0          1.0  \n",
       "9              0          4.0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Build flattened timetable with stop-level detail\n",
    "# ============================================================\n",
    "\n",
    "# Build a detailed timetable: for each journey, expand all stops with calculated arrival times\n",
    "timetable_rows = []\n",
    "\n",
    "for journey in all_journeys:\n",
    "    jp_section_ref = journey.get('journey_pattern_section_ref', '')\n",
    "    timing_links = all_jp_sections.get(jp_section_ref, [])\n",
    "    dep_time_str = journey.get('departure_time', '00:00:00')\n",
    "    \n",
    "    # Parse departure time to minutes from midnight\n",
    "    parts = dep_time_str.split(':')\n",
    "    if len(parts) >= 2:\n",
    "        dep_minutes = int(parts[0]) * 60 + int(parts[1])\n",
    "        if len(parts) == 3:\n",
    "            dep_minutes += int(parts[2]) / 60\n",
    "    else:\n",
    "        dep_minutes = 0\n",
    "    \n",
    "    cumulative_time = 0  # minutes from first departure\n",
    "    \n",
    "    for i, tl in enumerate(timing_links):\n",
    "        from_stop_ref = tl['from_stop']\n",
    "        to_stop_ref = tl['to_stop']\n",
    "        from_stop_info = all_stops.get(from_stop_ref, {})\n",
    "        to_stop_info = all_stops.get(to_stop_ref, {})\n",
    "        \n",
    "        # First stop in journey\n",
    "        if i == 0:\n",
    "            arrival_minutes = dep_minutes + cumulative_time\n",
    "            hrs, mins = divmod(int(arrival_minutes), 60)\n",
    "            timetable_rows.append({\n",
    "                **{k: journey[k] for k in [\n",
    "                    'file_name', 'vehicle_journey_code', 'journey_code',\n",
    "                    'departure_time', 'service_code', 'line_name', 'direction',\n",
    "                    'origin', 'destination', 'start_date', 'end_date',\n",
    "                    'days_of_week', 'operator_code', 'operator_name'\n",
    "                ]},\n",
    "                'stop_sequence': tl['from_seq'],\n",
    "                'stop_ref': from_stop_ref,\n",
    "                'stop_name': from_stop_info.get('stop_name', ''),\n",
    "                'longitude': from_stop_info.get('longitude', 0),\n",
    "                'latitude': from_stop_info.get('latitude', 0),\n",
    "                'scheduled_arrival': f\"{hrs:02d}:{mins:02d}:00\",\n",
    "                'wait_time_min': tl['from_wait_time'],\n",
    "                'run_time_min': tl['run_time'],\n",
    "            })\n",
    "            cumulative_time += tl['from_wait_time']\n",
    "        \n",
    "        # Add run time to get to the next stop\n",
    "        cumulative_time += tl['run_time']\n",
    "        arrival_minutes = dep_minutes + cumulative_time\n",
    "        hrs, mins = divmod(int(arrival_minutes), 60)\n",
    "        \n",
    "        timetable_rows.append({\n",
    "            **{k: journey[k] for k in [\n",
    "                'file_name', 'vehicle_journey_code', 'journey_code',\n",
    "                'departure_time', 'service_code', 'line_name', 'direction',\n",
    "                'origin', 'destination', 'start_date', 'end_date',\n",
    "                'days_of_week', 'operator_code', 'operator_name'\n",
    "            ]},\n",
    "            'stop_sequence': tl['to_seq'],\n",
    "            'stop_ref': to_stop_ref,\n",
    "            'stop_name': to_stop_info.get('stop_name', ''),\n",
    "            'longitude': to_stop_info.get('longitude', 0),\n",
    "            'latitude': to_stop_info.get('latitude', 0),\n",
    "            'scheduled_arrival': f\"{hrs:02d}:{mins:02d}:00\",\n",
    "            'wait_time_min': 0,\n",
    "            'run_time_min': tl['run_time'],\n",
    "        })\n",
    "\n",
    "timetable_df = pd.DataFrame(timetable_rows)\n",
    "print(f\"Timetable DataFrame shape: {timetable_df.shape}\")\n",
    "print(f\"\\nColumns: {list(timetable_df.columns)}\")\n",
    "timetable_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0186b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENSIGN BUS TIMETABLE DATA SUMMARY\n",
      "============================================================\n",
      "Total rows (journey-stop combinations): 34922\n",
      "Unique vehicle journeys: 79\n",
      "Unique bus lines: ['22' '33' '44' '73' '83' '88' '99OT' '99' 'x1' 'x2' 'x32' 'x80']\n",
      "Unique stops: 107\n",
      "Unique service codes: 12\n",
      "Date range: 2025-01-04 to 2026-03-27\n",
      "Directions: ['outbound' 'inbound' 'clockwise']\n",
      "\n",
      "Days of week breakdown:\n",
      "days_of_week\n",
      "Friday                                                            9094\n",
      "Thursday                                                          8482\n",
      "Monday, Tuesday, Wednesday                                        8462\n",
      "Monday, Tuesday, Wednesday, Thursday, Friday                      5054\n",
      "Saturday                                                          2504\n",
      "Sunday                                                            1042\n",
      "Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday     284\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Line breakdown:\n",
      "line_name\n",
      "22      79\n",
      "33      41\n",
      "44      66\n",
      "73      59\n",
      "83      61\n",
      "88      28\n",
      "99      30\n",
      "99OT    36\n",
      "x1      22\n",
      "x2      46\n",
      "x32     58\n",
      "x80     32\n",
      "Name: unique_journeys, dtype: int64\n",
      "\n",
      "Data types:\n",
      "file_name                object\n",
      "vehicle_journey_code     object\n",
      "journey_code             object\n",
      "departure_time           object\n",
      "service_code             object\n",
      "line_name                object\n",
      "direction                object\n",
      "origin                   object\n",
      "destination              object\n",
      "start_date               object\n",
      "end_date                 object\n",
      "days_of_week             object\n",
      "operator_code            object\n",
      "operator_name            object\n",
      "stop_sequence             int64\n",
      "stop_ref                 object\n",
      "stop_name                object\n",
      "longitude               float64\n",
      "latitude                float64\n",
      "scheduled_arrival        object\n",
      "wait_time_min             int64\n",
      "run_time_min            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Quick summary of timetable data\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENSIGN BUS TIMETABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows (journey-stop combinations): {len(timetable_df)}\")\n",
    "print(f\"Unique vehicle journeys: {timetable_df['vehicle_journey_code'].nunique()}\")\n",
    "print(f\"Unique bus lines: {timetable_df['line_name'].unique()}\")\n",
    "print(f\"Unique stops: {timetable_df['stop_ref'].nunique()}\")\n",
    "print(f\"Unique service codes: {timetable_df['service_code'].nunique()}\")\n",
    "print(f\"Date range: {timetable_df['start_date'].min()} to {timetable_df['end_date'].max()}\")\n",
    "print(f\"Directions: {timetable_df['direction'].unique()}\")\n",
    "print(f\"\\nDays of week breakdown:\")\n",
    "print(timetable_df['days_of_week'].value_counts())\n",
    "print(f\"\\nLine breakdown:\")\n",
    "print(timetable_df.groupby('line_name')['vehicle_journey_code'].nunique().rename('unique_journeys'))\n",
    "print(f\"\\nData types:\\n{timetable_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63f836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing SIRI-SX disruptions XML (this may take a moment, ~72k lines)...\n",
      "\n",
      "Disruptions DataFrame shape: (428, 18)\n",
      "Columns: ['situation_number', 'creation_time', 'participant_ref', 'progress', 'reason', 'planned', 'summary', 'description', 'validity_start', 'validity_end', 'severity', 'condition', 'advice', 'vehicle_mode', 'affected_operators', 'affected_lines', 'affected_stops', 'affected_places']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>situation_number</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>participant_ref</th>\n",
       "      <th>progress</th>\n",
       "      <th>reason</th>\n",
       "      <th>planned</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>validity_start</th>\n",
       "      <th>validity_end</th>\n",
       "      <th>severity</th>\n",
       "      <th>condition</th>\n",
       "      <th>advice</th>\n",
       "      <th>vehicle_mode</th>\n",
       "      <th>affected_operators</th>\n",
       "      <th>affected_lines</th>\n",
       "      <th>affected_stops</th>\n",
       "      <th>affected_places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18224249-29cf-4f65-a023-11067e7c2b6f</td>\n",
       "      <td>2024-08-30T09:07:04.813Z</td>\n",
       "      <td>WestofEngland</td>\n",
       "      <td>open</td>\n",
       "      <td>roadworks</td>\n",
       "      <td>true</td>\n",
       "      <td>Live Traffic Update: York Road One Way Closure</td>\n",
       "      <td>Long term works - York Road is closed eastboun...</td>\n",
       "      <td>2024-09-01T08:00:00.000Z</td>\n",
       "      <td></td>\n",
       "      <td>normal</td>\n",
       "      <td>unknown</td>\n",
       "      <td>York Road closed between St Luke's Road to the...</td>\n",
       "      <td>bus</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Bristol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b9e1d8f-b0ee-43a7-8ca5-7334d6fc4587</td>\n",
       "      <td>2024-11-01T15:42:15.905Z</td>\n",
       "      <td>WYCA</td>\n",
       "      <td>open</td>\n",
       "      <td>roadworks</td>\n",
       "      <td>true</td>\n",
       "      <td>Horsforth Vale, Bletchley Avenue, Bletchley Ro...</td>\n",
       "      <td>Bletchley Avenue, Bletchley Road and Low Hall ...</td>\n",
       "      <td>2024-11-04T08:30:00.000Z</td>\n",
       "      <td></td>\n",
       "      <td>slight</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Service 9 at 0838, 1553, 1658 &amp; 1733 towards W...</td>\n",
       "      <td>bus</td>\n",
       "      <td>YSQU:Squarepeg</td>\n",
       "      <td>9:9</td>\n",
       "      <td>450032047:Bletchley Avenue | 450032046:Bletchl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75e92a90-d4f1-4124-9cf4-633abacf8b81</td>\n",
       "      <td>2025-01-10T11:54:42.620Z</td>\n",
       "      <td>WYCA</td>\n",
       "      <td>open</td>\n",
       "      <td>roadClosed</td>\n",
       "      <td>true</td>\n",
       "      <td>King Cross, King Cross Road (Calderdale)</td>\n",
       "      <td>Due to new road layout on King Cross Road serv...</td>\n",
       "      <td>2025-01-10T11:50:00.000Z</td>\n",
       "      <td></td>\n",
       "      <td>slight</td>\n",
       "      <td>unknown</td>\n",
       "      <td>First services 579, 586, 590, 591 &amp; 592 are no...</td>\n",
       "      <td>bus</td>\n",
       "      <td>FHUD:FIRST WEST YORKSHIRE LTD | FHUD:FIRST WES...</td>\n",
       "      <td>579:579 | 586:586 | 587:587 | 590:590 | 591:59...</td>\n",
       "      <td>450023283:King Cross</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba174836-2698-4f06-8f22-e6c90aca9a7d</td>\n",
       "      <td>2025-03-06T09:16:59Z</td>\n",
       "      <td>WestofEngland</td>\n",
       "      <td>open</td>\n",
       "      <td>roadworks</td>\n",
       "      <td>false</td>\n",
       "      <td>Tower Road/Station Road</td>\n",
       "      <td>Due to Temporary Lights on the Junction of Tow...</td>\n",
       "      <td>2025-03-06T09:16:00.000Z</td>\n",
       "      <td></td>\n",
       "      <td>normal</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Due to Temporary Lights on the Junction of Tow...</td>\n",
       "      <td>bus</td>\n",
       "      <td>FBRI:First Bristol Limited</td>\n",
       "      <td>43:43</td>\n",
       "      <td>0170SGB20128:Baden Road</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8e8d2259-4d28-490b-9785-c08b0fd034bd</td>\n",
       "      <td>2025-03-28T11:17:43Z</td>\n",
       "      <td>WestofEngland</td>\n",
       "      <td>open</td>\n",
       "      <td>roadworks</td>\n",
       "      <td>true</td>\n",
       "      <td>Road Closure: Victoria Street, Bristol</td>\n",
       "      <td>Victoria Street in Bristol City Centre, northb...</td>\n",
       "      <td>2025-03-30T23:01:00.000Z</td>\n",
       "      <td></td>\n",
       "      <td>normal</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Temple Meads T7 will not be served during this...</td>\n",
       "      <td>bus</td>\n",
       "      <td>FBRI:First Bristol Limited | FBRI:First Bristo...</td>\n",
       "      <td>1:1 | 2:2 | 2a:2a | 39:39 | 72:72 | 172:172 | ...</td>\n",
       "      <td>0100BRP90311:Temple Meads Stn</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       situation_number             creation_time  \\\n",
       "0  18224249-29cf-4f65-a023-11067e7c2b6f  2024-08-30T09:07:04.813Z   \n",
       "1  2b9e1d8f-b0ee-43a7-8ca5-7334d6fc4587  2024-11-01T15:42:15.905Z   \n",
       "2  75e92a90-d4f1-4124-9cf4-633abacf8b81  2025-01-10T11:54:42.620Z   \n",
       "3  ba174836-2698-4f06-8f22-e6c90aca9a7d      2025-03-06T09:16:59Z   \n",
       "4  8e8d2259-4d28-490b-9785-c08b0fd034bd      2025-03-28T11:17:43Z   \n",
       "\n",
       "  participant_ref progress      reason planned  \\\n",
       "0   WestofEngland     open   roadworks    true   \n",
       "1            WYCA     open   roadworks    true   \n",
       "2            WYCA     open  roadClosed    true   \n",
       "3   WestofEngland     open   roadworks   false   \n",
       "4   WestofEngland     open   roadworks    true   \n",
       "\n",
       "                                             summary  \\\n",
       "0     Live Traffic Update: York Road One Way Closure   \n",
       "1  Horsforth Vale, Bletchley Avenue, Bletchley Ro...   \n",
       "2           King Cross, King Cross Road (Calderdale)   \n",
       "3                            Tower Road/Station Road   \n",
       "4             Road Closure: Victoria Street, Bristol   \n",
       "\n",
       "                                         description  \\\n",
       "0  Long term works - York Road is closed eastboun...   \n",
       "1  Bletchley Avenue, Bletchley Road and Low Hall ...   \n",
       "2  Due to new road layout on King Cross Road serv...   \n",
       "3  Due to Temporary Lights on the Junction of Tow...   \n",
       "4  Victoria Street in Bristol City Centre, northb...   \n",
       "\n",
       "             validity_start validity_end severity condition  \\\n",
       "0  2024-09-01T08:00:00.000Z                normal   unknown   \n",
       "1  2024-11-04T08:30:00.000Z                slight   unknown   \n",
       "2  2025-01-10T11:50:00.000Z                slight   unknown   \n",
       "3  2025-03-06T09:16:00.000Z                normal   unknown   \n",
       "4  2025-03-30T23:01:00.000Z                normal   unknown   \n",
       "\n",
       "                                              advice vehicle_mode  \\\n",
       "0  York Road closed between St Luke's Road to the...          bus   \n",
       "1  Service 9 at 0838, 1553, 1658 & 1733 towards W...          bus   \n",
       "2  First services 579, 586, 590, 591 & 592 are no...          bus   \n",
       "3  Due to Temporary Lights on the Junction of Tow...          bus   \n",
       "4  Temple Meads T7 will not be served during this...          bus   \n",
       "\n",
       "                                  affected_operators  \\\n",
       "0                                                      \n",
       "1                                     YSQU:Squarepeg   \n",
       "2  FHUD:FIRST WEST YORKSHIRE LTD | FHUD:FIRST WES...   \n",
       "3                         FBRI:First Bristol Limited   \n",
       "4  FBRI:First Bristol Limited | FBRI:First Bristo...   \n",
       "\n",
       "                                      affected_lines  \\\n",
       "0                                                      \n",
       "1                                                9:9   \n",
       "2  579:579 | 586:586 | 587:587 | 590:590 | 591:59...   \n",
       "3                                              43:43   \n",
       "4  1:1 | 2:2 | 2a:2a | 39:39 | 72:72 | 172:172 | ...   \n",
       "\n",
       "                                      affected_stops affected_places  \n",
       "0                                                            Bristol  \n",
       "1  450032047:Bletchley Avenue | 450032046:Bletchl...                  \n",
       "2                               450023283:King Cross                  \n",
       "3                            0170SGB20128:Baden Road                  \n",
       "4                      0100BRP90311:Temple Meads Stn                  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Parse SIRI-SX Disruptions Data\n",
    "# ============================================================\n",
    "\n",
    "SIRI_NS = {'siri': 'http://www.siri.org.uk/siri'}\n",
    "\n",
    "sirisx_path = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\raw\\sirisx_2026-02-06_145256\\sirisx.xml'\n",
    "\n",
    "print(\"Parsing SIRI-SX disruptions XML (this may take a moment, ~72k lines)...\")\n",
    "\n",
    "tree = ET.parse(sirisx_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "disruption_rows = []\n",
    "\n",
    "for sit in root.findall('.//siri:PtSituationElement', SIRI_NS):\n",
    "    # Base disruption info\n",
    "    situation_number = sit.findtext('siri:SituationNumber', '', SIRI_NS)\n",
    "    creation_time = sit.findtext('siri:CreationTime', '', SIRI_NS)\n",
    "    participant_ref = sit.findtext('siri:ParticipantRef', '', SIRI_NS)\n",
    "    progress = sit.findtext('siri:Progress', '', SIRI_NS)\n",
    "    reason = sit.findtext('siri:MiscellaneousReason', '', SIRI_NS)\n",
    "    if not reason:\n",
    "        reason = sit.findtext('siri:EnvironmentReason', '', SIRI_NS)\n",
    "    if not reason:\n",
    "        reason = sit.findtext('siri:EquipmentReason', '', SIRI_NS)\n",
    "    if not reason:\n",
    "        reason = sit.findtext('siri:PersonnelReason', '', SIRI_NS)\n",
    "    planned = sit.findtext('siri:Planned', '', SIRI_NS)\n",
    "    summary = sit.findtext('siri:Summary', '', SIRI_NS)\n",
    "    description = sit.findtext('siri:Description', '', SIRI_NS)\n",
    "    \n",
    "    # Validity period\n",
    "    validity_start = sit.findtext('.//siri:ValidityPeriod/siri:StartTime', '', SIRI_NS)\n",
    "    validity_end = sit.findtext('.//siri:ValidityPeriod/siri:EndTime', '', SIRI_NS)\n",
    "    \n",
    "    # Process each Consequence\n",
    "    for conseq in sit.findall('.//siri:Consequences/siri:Consequence', SIRI_NS):\n",
    "        severity = conseq.findtext('siri:Severity', '', SIRI_NS)\n",
    "        condition = conseq.findtext('siri:Condition', '', SIRI_NS)\n",
    "        advice = conseq.findtext('.//siri:Advice/siri:Details', '', SIRI_NS)\n",
    "        \n",
    "        # Get affected operators and lines\n",
    "        affected_operators = []\n",
    "        affected_lines = []\n",
    "        for network in conseq.findall('.//siri:AffectedNetwork', SIRI_NS):\n",
    "            vehicle_mode = network.findtext('siri:VehicleMode', '', SIRI_NS)\n",
    "            for aline in network.findall('siri:AffectedLine', SIRI_NS):\n",
    "                op_ref = aline.findtext('.//siri:OperatorRef', '', SIRI_NS)\n",
    "                op_name = aline.findtext('.//siri:OperatorName', '', SIRI_NS)\n",
    "                line_ref = aline.findtext('siri:LineRef', '', SIRI_NS)\n",
    "                line_name = aline.findtext('siri:PublishedLineName', '', SIRI_NS)\n",
    "                affected_operators.append(f\"{op_ref}:{op_name}\")\n",
    "                affected_lines.append(f\"{line_ref}:{line_name}\")\n",
    "        \n",
    "        # Get affected stop points\n",
    "        affected_stops = []\n",
    "        for sp in conseq.findall('.//siri:AffectedStopPoint', SIRI_NS):\n",
    "            sp_ref = sp.findtext('siri:StopPointRef', '', SIRI_NS)\n",
    "            sp_name = sp.findtext('siri:StopPointName', '', SIRI_NS)\n",
    "            affected_stops.append(f\"{sp_ref}:{sp_name}\")\n",
    "        \n",
    "        # Get affected places\n",
    "        affected_places = []\n",
    "        for place in conseq.findall('.//siri:AffectedPlace', SIRI_NS):\n",
    "            place_name = place.findtext('siri:PlaceName', '', SIRI_NS)\n",
    "            if place_name:\n",
    "                affected_places.append(place_name)\n",
    "        \n",
    "        disruption_rows.append({\n",
    "            'situation_number': situation_number,\n",
    "            'creation_time': creation_time,\n",
    "            'participant_ref': participant_ref,\n",
    "            'progress': progress,\n",
    "            'reason': reason,\n",
    "            'planned': planned,\n",
    "            'summary': summary,\n",
    "            'description': description,\n",
    "            'validity_start': validity_start,\n",
    "            'validity_end': validity_end,\n",
    "            'severity': severity,\n",
    "            'condition': condition,\n",
    "            'advice': advice,\n",
    "            'vehicle_mode': vehicle_mode if 'vehicle_mode' in dir() else '',\n",
    "            'affected_operators': ' | '.join(affected_operators) if affected_operators else '',\n",
    "            'affected_lines': ' | '.join(affected_lines) if affected_lines else '',\n",
    "            'affected_stops': ' | '.join(affected_stops) if affected_stops else '',\n",
    "            'affected_places': ' | '.join(affected_places) if affected_places else '',\n",
    "        })\n",
    "\n",
    "disruptions_df = pd.DataFrame(disruption_rows)\n",
    "print(f\"\\nDisruptions DataFrame shape: {disruptions_df.shape}\")\n",
    "print(f\"Columns: {list(disruptions_df.columns)}\")\n",
    "disruptions_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81ff9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SIRI-SX DISRUPTIONS DATA SUMMARY\n",
      "============================================================\n",
      "Total disruption records: 428\n",
      "Unique situations: 349\n",
      "\n",
      "Disruption reasons:\n",
      "reason\n",
      "roadworks                   212\n",
      "maintenanceWork             116\n",
      "roadClosed                   50\n",
      "routeDiversion               11\n",
      "specialEvent                 10\n",
      "incident                      7\n",
      "unknown                       6\n",
      "repairWork                    5\n",
      "liftFailure                   3\n",
      "emergencyEngineeringWork      2\n",
      "insufficientDemand            2\n",
      "vandalism                     2\n",
      "constructionWork              1\n",
      "escalatorFailure              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Severity levels:\n",
      "severity\n",
      "normal        217\n",
      "slight        134\n",
      "unknown        46\n",
      "severe         18\n",
      "verySevere     10\n",
      "verySlight      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Planned vs Unplanned:\n",
      "planned\n",
      "true     354\n",
      "false     74\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Progress status:\n",
      "progress\n",
      "open    428\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 affected places:\n",
      "affected_places\n",
      "South        5\n",
      "Yorkshire    5\n",
      "Bristol      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: SIRI-SX Disruptions Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIRI-SX DISRUPTIONS DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total disruption records: {len(disruptions_df)}\")\n",
    "print(f\"Unique situations: {disruptions_df['situation_number'].nunique()}\")\n",
    "print(f\"\\nDisruption reasons:\")\n",
    "print(disruptions_df['reason'].value_counts())\n",
    "print(f\"\\nSeverity levels:\")\n",
    "print(disruptions_df['severity'].value_counts())\n",
    "print(f\"\\nPlanned vs Unplanned:\")\n",
    "print(disruptions_df['planned'].value_counts())\n",
    "print(f\"\\nProgress status:\")\n",
    "print(disruptions_df['progress'].value_counts())\n",
    "print(f\"\\nTop 10 affected places:\")\n",
    "# Explode the places to see distribution\n",
    "places = disruptions_df['affected_places'].str.split(' | ').explode().dropna()\n",
    "places = places[places != '']\n",
    "print(places.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88406ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MERGE FEASIBILITY CHECK\n",
      "============================================================\n",
      "\n",
      "Timetable operator codes: {'ENSB'}\n",
      "Disruptions mentioning ENSB (Ensign Bus): 0\n",
      "\n",
      "Timetable line names: {'22', '44', 'x32', '73', 'x1', 'x2', '83', 'x80', '33', '99OT', '88', '99'}\n",
      "Matching line names: {'22', '44', '73', '83', '33', '88', '99'}\n",
      "\n",
      "Timetable unique stops: 107\n",
      "Disruption unique stops: 2663\n",
      "Matching stop refs: 0\n",
      "\n",
      "============================================================\n",
      "RESULT: Data CAN be merged! Common fields found.\n",
      "  -> Merge via line_name (7 matching lines)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Check if Timetable & Disruption data can be merged\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGE FEASIBILITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Check 1: Operator match ---\n",
    "# Ensign Bus operator code from timetable\n",
    "timetable_operators = set(timetable_df['operator_code'].unique())\n",
    "print(f\"\\nTimetable operator codes: {timetable_operators}\")\n",
    "\n",
    "# Check if ENSB appears in disruptions\n",
    "ensb_disruptions = disruptions_df[\n",
    "    disruptions_df['affected_operators'].str.contains('ENSB', case=False, na=False)\n",
    "]\n",
    "print(f\"Disruptions mentioning ENSB (Ensign Bus): {len(ensb_disruptions)}\")\n",
    "\n",
    "# --- Check 2: Line name match ---\n",
    "timetable_lines = set(timetable_df['line_name'].unique())\n",
    "print(f\"\\nTimetable line names: {timetable_lines}\")\n",
    "\n",
    "# Extract line names from disruptions\n",
    "all_disruption_lines = set()\n",
    "for lines_str in disruptions_df['affected_lines'].dropna():\n",
    "    for line in lines_str.split(' | '):\n",
    "        parts = line.split(':')\n",
    "        if len(parts) >= 2:\n",
    "            all_disruption_lines.add(parts[1])\n",
    "\n",
    "matching_lines = timetable_lines & all_disruption_lines\n",
    "print(f\"Matching line names: {matching_lines if matching_lines else 'NONE'}\")\n",
    "\n",
    "# --- Check 3: Stop reference match ---\n",
    "timetable_stops = set(timetable_df['stop_ref'].unique())\n",
    "disruption_stops = set()\n",
    "for stops_str in disruptions_df['affected_stops'].dropna():\n",
    "    for stop in stops_str.split(' | '):\n",
    "        parts = stop.split(':')\n",
    "        if parts[0]:\n",
    "            disruption_stops.add(parts[0])\n",
    "\n",
    "matching_stops = timetable_stops & disruption_stops\n",
    "print(f\"\\nTimetable unique stops: {len(timetable_stops)}\")\n",
    "print(f\"Disruption unique stops: {len(disruption_stops)}\")\n",
    "print(f\"Matching stop refs: {len(matching_stops)}\")\n",
    "if matching_stops:\n",
    "    print(f\"Matched stops: {list(matching_stops)[:20]}\")\n",
    "\n",
    "# --- Decision ---\n",
    "can_merge = len(ensb_disruptions) > 0 or len(matching_stops) > 0 or len(matching_lines) > 0\n",
    "print(f\"\\n{'='*60}\")\n",
    "if can_merge:\n",
    "    print(\"RESULT: Data CAN be merged! Common fields found.\")\n",
    "    if len(matching_stops) > 0:\n",
    "        print(f\"  -> Merge via stop_ref ({len(matching_stops)} matching stops)\")\n",
    "    if len(ensb_disruptions) > 0:\n",
    "        print(f\"  -> Merge via operator_code ({len(ensb_disruptions)} ENSB disruptions)\")\n",
    "    if len(matching_lines) > 0:\n",
    "        print(f\"  -> Merge via line_name ({len(matching_lines)} matching lines)\")\n",
    "else:\n",
    "    print(\"RESULT: No direct overlap found between Ensign Bus timetable and SIRI-SX disruptions.\")\n",
    "    print(\"  The disruptions data covers different operators/regions.\")\n",
    "    print(\"  Both datasets will be saved separately as CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f224bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No direct merge possible. Saving datasets separately.\n",
      "Both CSVs will be exported in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Merge if possible, otherwise keep separate\n",
    "# ============================================================\n",
    "\n",
    "if can_merge and len(matching_stops) > 0:\n",
    "    # Explode disruptions by affected stops for a stop-level merge\n",
    "    disrupt_exploded = disruptions_df.copy()\n",
    "    disrupt_exploded['affected_stops_list'] = disrupt_exploded['affected_stops'].str.split(' | ')\n",
    "    disrupt_exploded = disrupt_exploded.explode('affected_stops_list')\n",
    "    disrupt_exploded['disrupt_stop_ref'] = disrupt_exploded['affected_stops_list'].str.split(':').str[0]\n",
    "    \n",
    "    # Merge on stop_ref\n",
    "    merged_df = timetable_df.merge(\n",
    "        disrupt_exploded[['situation_number', 'reason', 'planned', 'severity',\n",
    "                          'summary', 'description', 'validity_start', 'validity_end',\n",
    "                          'disrupt_stop_ref']],\n",
    "        left_on='stop_ref',\n",
    "        right_on='disrupt_stop_ref',\n",
    "        how='left',\n",
    "        suffixes=('', '_disruption')\n",
    "    )\n",
    "    merged_df['has_disruption'] = merged_df['situation_number'].notna()\n",
    "    \n",
    "    print(f\"Merged DataFrame shape: {merged_df.shape}\")\n",
    "    print(f\"Journeys with disruptions: {merged_df['has_disruption'].sum()}\")\n",
    "    print(f\"Journeys without disruptions: {(~merged_df['has_disruption']).sum()}\")\n",
    "    \n",
    "    # Save merged\n",
    "    output_dir = r'F:\\SOFTWARICA\\big-data-transport-analytics\\outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    merged_df.to_csv(os.path.join(output_dir, 'ensign_timetable_with_disruptions.csv'), index=False)\n",
    "    print(f\"\\nSaved: outputs/ensign_timetable_with_disruptions.csv\")\n",
    "    merged_df.head()\n",
    "else:\n",
    "    print(\"No direct merge possible. Saving datasets separately.\")\n",
    "    print(\"Both CSVs will be exported in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce80b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ensign_bus_timetable.csv (34922 rows x 22 cols)\n",
      "Saved: ensign_bus_stops.csv (505 rows x 4 cols)\n",
      "Saved: sirisx_disruptions.csv (428 rows x 18 cols)\n",
      "Saved: ensign_bus_journeys.csv (3986 rows x 17 cols)\n",
      "\n",
      "All CSVs saved to: F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed\n",
      "\n",
      "File sizes:\n",
      "  ensign_bus_journeys.csv: 839.5 KB (0.82 MB)\n",
      "  ensign_bus_stops.csv: 22.7 KB (0.02 MB)\n",
      "  ensign_bus_timetable.csv: 8648.1 KB (8.45 MB)\n",
      "  sirisx_disruptions.csv: 351.6 KB (0.34 MB)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Export all DataFrames to CSV\n",
    "# ============================================================\n",
    "\n",
    "output_dir = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Timetable CSV\n",
    "timetable_csv = os.path.join(output_dir, 'ensign_bus_timetable.csv')\n",
    "timetable_df.to_csv(timetable_csv, index=False)\n",
    "print(f\"Saved: ensign_bus_timetable.csv ({timetable_df.shape[0]} rows x {timetable_df.shape[1]} cols)\")\n",
    "\n",
    "# 2. Stops CSV\n",
    "stops_df = pd.DataFrame(all_stops.values())\n",
    "stops_csv = os.path.join(output_dir, 'ensign_bus_stops.csv')\n",
    "stops_df.to_csv(stops_csv, index=False)\n",
    "print(f\"Saved: ensign_bus_stops.csv ({stops_df.shape[0]} rows x {stops_df.shape[1]} cols)\")\n",
    "\n",
    "# 3. Disruptions CSV\n",
    "disruptions_csv = os.path.join(output_dir, 'sirisx_disruptions.csv')\n",
    "disruptions_df.to_csv(disruptions_csv, index=False)\n",
    "print(f\"Saved: sirisx_disruptions.csv ({disruptions_df.shape[0]} rows x {disruptions_df.shape[1]} cols)\")\n",
    "\n",
    "# 4. Vehicle Journeys summary CSV (one row per journey, without stop expansion)\n",
    "journeys_df = pd.DataFrame(all_journeys)\n",
    "journeys_csv = os.path.join(output_dir, 'ensign_bus_journeys.csv')\n",
    "journeys_df.to_csv(journeys_csv, index=False)\n",
    "print(f\"Saved: ensign_bus_journeys.csv ({journeys_df.shape[0]} rows x {journeys_df.shape[1]} cols)\")\n",
    "\n",
    "print(f\"\\nAll CSVs saved to: {output_dir}\")\n",
    "print(\"\\nFile sizes:\")\n",
    "for f in os.listdir(output_dir):\n",
    "    if f.endswith('.csv'):\n",
    "        size = os.path.getsize(os.path.join(output_dir, f))\n",
    "        print(f\"  {f}: {size/1024:.1f} KB ({size/1024/1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "990b8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMETABLE DATA PREVIEW (first 5 rows)\n",
      "================================================================================\n",
      "line_name direction vehicle_journey_code departure_time  stop_sequence                stop_name scheduled_arrival               days_of_week operator_name start_date   end_date\n",
      "       22  outbound                 vj_1       17:43:00              1 Chafford Hundred Station          17:43:00 Monday, Tuesday, Wednesday     Ensignbus 2026-01-05 2026-02-11\n",
      "       22  outbound                 vj_1       17:43:00              2             Fleming Road          17:44:00 Monday, Tuesday, Wednesday     Ensignbus 2026-01-05 2026-02-11\n",
      "       22  outbound                 vj_1       17:43:00              3  Lakeside Bus Station, N          17:48:00 Monday, Tuesday, Wednesday     Ensignbus 2026-01-05 2026-02-11\n",
      "       22  outbound                 vj_1       17:43:00              4                    Tesco          17:50:00 Monday, Tuesday, Wednesday     Ensignbus 2026-01-05 2026-02-11\n",
      "       22  outbound                 vj_1       17:43:00              5        Galleon Boulevard          18:00:00 Monday, Tuesday, Wednesday     Ensignbus 2026-01-05 2026-02-11\n",
      "\n",
      "================================================================================\n",
      "DISRUPTIONS DATA PREVIEW (first 5 rows)\n",
      "================================================================================\n",
      "                    situation_number     reason severity planned                                                                    summary                                                                                                                                                                                                                                                                                                                                                                                          affected_operators                                                                                                              affected_lines\n",
      "18224249-29cf-4f65-a023-11067e7c2b6f  roadworks   normal    true                             Live Traffic Update: York Road One Way Closure                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "2b9e1d8f-b0ee-43a7-8ca5-7334d6fc4587  roadworks   slight    true Horsforth Vale, Bletchley Avenue, Bletchley Road and Low Hall Road (Leeds)                                                                                                                                                                                                                                                                                                                                                                                              YSQU:Squarepeg                                                                                                                         9:9\n",
      "75e92a90-d4f1-4124-9cf4-633abacf8b81 roadClosed   slight    true                                   King Cross, King Cross Road (Calderdale)                                                                                                                                                                                                                           FHUD:FIRST WEST YORKSHIRE LTD | FHUD:FIRST WEST YORKSHIRE LTD | TPEN:Team Pennine | FHUD:FIRST WEST YORKSHIRE LTD | FHUD:FIRST WEST YORKSHIRE LTD | FHUD:FIRST WEST YORKSHIRE LTD                                                                   579:579 | 586:586 | 587:587 | 590:590 | 591:591 | 592:592\n",
      "ba174836-2698-4f06-8f22-e6c90aca9a7d  roadworks   normal   false                                                    Tower Road/Station Road                                                                                                                                                                                                                                                                                                                                                                                  FBRI:First Bristol Limited                                                                                                                       43:43\n",
      "8e8d2259-4d28-490b-9785-c08b0fd034bd  roadworks   normal    true                                     Road Closure: Victoria Street, Bristol FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | FBRI:First Bristol Limited | LEMB:The Big Lemon | FBRI:First Bristol Limited 1:1 | 2:2 | 2a:2a | 39:39 | 72:72 | 172:172 | 349:349 | 374:374 | 375:375 | 376:376 | 376a:376a | 522:522 | P1:P1 | X39:X39\n",
      "\n",
      "================================================================================\n",
      "DATA READY FOR ANALYSIS!\n",
      "================================================================================\n",
      "  Timetable: 34,922 rows | 12 lines | 107 stops\n",
      "  Disruptions: 428 records | 14 reason types\n",
      "\n",
      "  CSVs available in: outputs/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Final data preview - Timetable & Disruptions side by side\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMETABLE DATA PREVIEW (first 5 rows)\")\n",
    "print(\"=\" * 80)\n",
    "display_cols_tt = ['line_name', 'direction', 'vehicle_journey_code', 'departure_time',\n",
    "                   'stop_sequence', 'stop_name', 'scheduled_arrival', 'days_of_week',\n",
    "                   'operator_name', 'start_date', 'end_date']\n",
    "print(timetable_df[display_cols_tt].head(5).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"DISRUPTIONS DATA PREVIEW (first 5 rows)\")\n",
    "print(\"=\" * 80)\n",
    "display_cols_dis = ['situation_number', 'reason', 'severity', 'planned', 'summary',\n",
    "                    'affected_operators', 'affected_lines']\n",
    "print(disruptions_df[display_cols_dis].head(5).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"DATA READY FOR ANALYSIS!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Timetable: {timetable_df.shape[0]:,} rows | {timetable_df['line_name'].nunique()} lines | {timetable_df['stop_ref'].nunique()} stops\")\n",
    "print(f\"  Disruptions: {disruptions_df.shape[0]:,} records | {disruptions_df['reason'].nunique()} reason types\")\n",
    "print(f\"\\n  CSVs available in: outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f8814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEEP ANALYSIS: Finding a viable merge strategy\n",
      "======================================================================\n",
      "\n",
      "Ensign Bus operating area:\n",
      "  Latitude range:  51.2326 to 51.5133\n",
      "  Longitude range: -0.3302 to 0.3769\n",
      "  (Thurrock / South Essex area)\n",
      "\n",
      "--- Re-parsing SIRI-SX for stop coordinates ---\n",
      "Disruption stops with coordinates: 3342\n",
      "\n",
      "Disruption stops within ~15km of Ensign Bus area:\n",
      "  Bounding box: lat [51.0826, 51.6633], lon [-0.5802, 0.6269]\n",
      "  Found: 0 disruption-stop records\n",
      "  Unique disruptions: 0\n",
      "\n",
      "  No geographically nearby disruptions found.\n",
      "  Expanding search to wider Greater London / Essex region...\n",
      "  Wider area (Â±55km): 0 disruption-stop records, 0 unique disruptions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Deep-dive - Can we actually merge these datasets?\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEEP ANALYSIS: Finding a viable merge strategy\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Ensign Bus operating area (from timetable stops) ---\n",
    "tt_lats = timetable_df['latitude'].values\n",
    "tt_lons = timetable_df['longitude'].values\n",
    "print(f\"\\nEnsign Bus operating area:\")\n",
    "print(f\"  Latitude range:  {tt_lats.min():.4f} to {tt_lats.max():.4f}\")\n",
    "print(f\"  Longitude range: {tt_lons.min():.4f} to {tt_lons.max():.4f}\")\n",
    "print(f\"  (Thurrock / South Essex area)\")\n",
    "\n",
    "# --- Extract lat/lon from disruption affected stops ---\n",
    "disrupt_stop_coords = []\n",
    "for _, row in disruptions_df.iterrows():\n",
    "    if pd.isna(row['affected_stops']) or row['affected_stops'] == '':\n",
    "        continue\n",
    "    for stop in row['affected_stops'].split(' | '):\n",
    "        parts = stop.split(':')\n",
    "        if len(parts) >= 2:\n",
    "            disrupt_stop_coords.append({\n",
    "                'situation_number': row['situation_number'],\n",
    "                'stop_ref': parts[0],\n",
    "                'stop_name': parts[1] if len(parts) > 1 else '',\n",
    "            })\n",
    "\n",
    "# We need to re-parse SIRI-SX to get stop coordinates from disruptions\n",
    "print(f\"\\n--- Re-parsing SIRI-SX for stop coordinates ---\")\n",
    "disrupt_coords = []\n",
    "for sit in root.findall('.//siri:PtSituationElement', SIRI_NS):\n",
    "    sit_id = sit.findtext('siri:SituationNumber', '', SIRI_NS)\n",
    "    for sp in sit.findall('.//siri:AffectedStopPoint', SIRI_NS):\n",
    "        sp_ref = sp.findtext('siri:StopPointRef', '', SIRI_NS)\n",
    "        sp_name = sp.findtext('siri:StopPointName', '', SIRI_NS)\n",
    "        lon_el = sp.findtext('siri:Location/siri:Longitude', '', SIRI_NS)\n",
    "        lat_el = sp.findtext('siri:Location/siri:Latitude', '', SIRI_NS)\n",
    "        if lon_el and lat_el:\n",
    "            disrupt_coords.append({\n",
    "                'situation_number': sit_id,\n",
    "                'disrupt_stop_ref': sp_ref,\n",
    "                'disrupt_stop_name': sp_name or '',\n",
    "                'disrupt_lon': float(lon_el),\n",
    "                'disrupt_lat': float(lat_el),\n",
    "            })\n",
    "\n",
    "disrupt_coords_df = pd.DataFrame(disrupt_coords)\n",
    "print(f\"Disruption stops with coordinates: {len(disrupt_coords_df)}\")\n",
    "\n",
    "# --- Geographic proximity check ---\n",
    "# Haversine approximation: at ~51.5Â°N, 1 degree lat â‰ˆ 111 km, 1 degree lon â‰ˆ 69 km\n",
    "# Use a bounding box: Ensign Bus area Â± 0.15 degrees (~15-17 km buffer)\n",
    "LAT_BUFFER = 0.15\n",
    "LON_BUFFER = 0.25\n",
    "\n",
    "lat_min, lat_max = tt_lats.min() - LAT_BUFFER, tt_lats.max() + LAT_BUFFER\n",
    "lon_min, lon_max = tt_lons.min() - LON_BUFFER, tt_lons.max() + LON_BUFFER\n",
    "\n",
    "nearby_disruptions = disrupt_coords_df[\n",
    "    (disrupt_coords_df['disrupt_lat'] >= lat_min) &\n",
    "    (disrupt_coords_df['disrupt_lat'] <= lat_max) &\n",
    "    (disrupt_coords_df['disrupt_lon'] >= lon_min) &\n",
    "    (disrupt_coords_df['disrupt_lon'] <= lon_max)\n",
    "]\n",
    "\n",
    "print(f\"\\nDisruption stops within ~15km of Ensign Bus area:\")\n",
    "print(f\"  Bounding box: lat [{lat_min:.4f}, {lat_max:.4f}], lon [{lon_min:.4f}, {lon_max:.4f}]\")\n",
    "print(f\"  Found: {len(nearby_disruptions)} disruption-stop records\")\n",
    "print(f\"  Unique disruptions: {nearby_disruptions['situation_number'].nunique()}\")\n",
    "\n",
    "if len(nearby_disruptions) > 0:\n",
    "    print(f\"\\n  Nearby disruption stops:\")\n",
    "    for _, r in nearby_disruptions.drop_duplicates('disrupt_stop_ref').head(20).iterrows():\n",
    "        print(f\"    {r['disrupt_stop_ref']}: {r['disrupt_stop_name']} ({r['disrupt_lat']:.4f}, {r['disrupt_lon']:.4f})\")\n",
    "else:\n",
    "    print(\"\\n  No geographically nearby disruptions found.\")\n",
    "    print(\"  Expanding search to wider Greater London / Essex region...\")\n",
    "    \n",
    "    # Wider search: Greater London + Essex\n",
    "    LAT_BUFFER2 = 0.5   # ~55 km\n",
    "    LON_BUFFER2 = 0.8   # ~55 km\n",
    "    lat_min2, lat_max2 = tt_lats.min() - LAT_BUFFER2, tt_lats.max() + LAT_BUFFER2\n",
    "    lon_min2, lon_max2 = tt_lons.min() - LON_BUFFER2, tt_lons.max() + LON_BUFFER2\n",
    "    \n",
    "    wider_disruptions = disrupt_coords_df[\n",
    "        (disrupt_coords_df['disrupt_lat'] >= lat_min2) &\n",
    "        (disrupt_coords_df['disrupt_lat'] <= lat_max2) &\n",
    "        (disrupt_coords_df['disrupt_lon'] >= lon_min2) &\n",
    "        (disrupt_coords_df['disrupt_lon'] <= lon_max2)\n",
    "    ]\n",
    "    print(f\"  Wider area (Â±55km): {len(wider_disruptions)} disruption-stop records, {wider_disruptions['situation_number'].nunique()} unique disruptions\")\n",
    "    if len(wider_disruptions) > 0:\n",
    "        print(f\"\\n  Sample wider disruption stops:\")\n",
    "        for _, r in wider_disruptions.drop_duplicates('disrupt_stop_ref').head(15).iterrows():\n",
    "            print(f\"    {r['disrupt_stop_ref']}: {r['disrupt_stop_name']} ({r['disrupt_lat']:.4f}, {r['disrupt_lon']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4c424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are the disruptions located?\n",
      "Total disruption stops with coords: 3342\n",
      "\n",
      "Latitude range:  50.1274 to 53.9049\n",
      "Longitude range: -5.5273 to -0.3289\n",
      "\n",
      "Ensign Bus area for comparison:\n",
      "  Lat: 51.2326 to 51.5133\n",
      "  Lon: -0.3302 to 0.3769\n",
      "\n",
      "Disruption sources (ParticipantRef):\n",
      "participant_ref\n",
      "TfGM                 144\n",
      "WestofEngland        110\n",
      "WYCA                  65\n",
      "Merseytravel          43\n",
      "SYMCA                 41\n",
      "Cornwall              19\n",
      "NorthLincolnshire      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Affected places:\n",
      "affected_places\n",
      "South Yorkshire    5\n",
      "Bristol            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Disruptions WITH affected stops: 387\n",
      "Disruptions WITHOUT affected stops: 41\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Where are the disruptions geographically?\n",
    "# ============================================================\n",
    "\n",
    "print(\"Where are the disruptions located?\")\n",
    "print(f\"Total disruption stops with coords: {len(disrupt_coords_df)}\")\n",
    "print(f\"\\nLatitude range:  {disrupt_coords_df['disrupt_lat'].min():.4f} to {disrupt_coords_df['disrupt_lat'].max():.4f}\")\n",
    "print(f\"Longitude range: {disrupt_coords_df['disrupt_lon'].min():.4f} to {disrupt_coords_df['disrupt_lon'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nEnsign Bus area for comparison:\")\n",
    "print(f\"  Lat: {tt_lats.min():.4f} to {tt_lats.max():.4f}\")\n",
    "print(f\"  Lon: {tt_lons.min():.4f} to {tt_lons.max():.4f}\")\n",
    "\n",
    "# Check participant_ref (source regions) in disruptions\n",
    "print(f\"\\nDisruption sources (ParticipantRef):\")\n",
    "print(disruptions_df['participant_ref'].value_counts())\n",
    "\n",
    "# Check the affected_places\n",
    "print(f\"\\nAffected places:\")\n",
    "places_all = disruptions_df['affected_places'].str.split(' \\\\| ').explode().dropna()\n",
    "places_all = places_all[places_all != '']\n",
    "print(places_all.value_counts())\n",
    "\n",
    "# How many disruption records have no stop coordinates?\n",
    "has_stops = disruptions_df['affected_stops'].str.len() > 0\n",
    "print(f\"\\nDisruptions WITH affected stops: {has_stops.sum()}\")\n",
    "print(f\"Disruptions WITHOUT affected stops: {(~has_stops).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5165333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timetable date range: 2025-01-04 to 2026-03-27\n",
      "\n",
      "Daily disruption features: (448, 11)\n",
      "\n",
      "Sample:\n",
      "      date  active_disruptions  unique_situations  avg_severity_score  max_severity_score  planned_count  unplanned_count  roadworks_count  roadclosed_count  other_reason_count    regions_affected\n",
      "2025-01-04                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-05                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-06                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-07                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-08                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-09                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-10                   2                  2            1.500000                 2.0              2                0                2                 0                   0 WestofEngland, WYCA\n",
      "2025-01-11                   3                  3            1.333333                 2.0              3                0                2                 1                   0 WestofEngland, WYCA\n",
      "2025-01-12                   3                  3            1.333333                 2.0              3                0                2                 1                   0 WestofEngland, WYCA\n",
      "2025-01-13                   3                  3            1.333333                 2.0              3                0                2                 1                   0 WestofEngland, WYCA\n",
      "\n",
      "\n",
      "Disruption count stats:\n",
      "count    448.000000\n",
      "mean      33.845982\n",
      "std       44.153190\n",
      "min        2.000000\n",
      "25%        8.000000\n",
      "50%       18.000000\n",
      "75%       36.000000\n",
      "max      210.000000\n",
      "Name: active_disruptions, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Build a TEMPORAL merge - disruption context per date\n",
    "# ============================================================\n",
    "# Strategy: The SIRI-SX data covers different regions but the SAME dates.\n",
    "# We create daily disruption features (count, severity, reasons) and\n",
    "# join them to timetable dates. This captures \"national disruption climate\"\n",
    "# which is valid for transport analytics (network knock-on effects).\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# --- Step 1: Build disruption date features ---\n",
    "# Parse disruption validity start dates\n",
    "disruptions_df['validity_start_dt'] = pd.to_datetime(\n",
    "    disruptions_df['validity_start'], errors='coerce', utc=True\n",
    ")\n",
    "disruptions_df['validity_end_dt'] = pd.to_datetime(\n",
    "    disruptions_df['validity_end'], errors='coerce', utc=True\n",
    ")\n",
    "disruptions_df['disruption_date'] = disruptions_df['validity_start_dt'].dt.date\n",
    "\n",
    "# --- Step 2: Parse timetable dates ---\n",
    "# Each timetable file covers a date range (start_date to end_date)\n",
    "# We'll expand to individual dates each journey operates on\n",
    "timetable_df['start_date_dt'] = pd.to_datetime(timetable_df['start_date'], errors='coerce')\n",
    "timetable_df['end_date_dt'] = pd.to_datetime(timetable_df['end_date'], errors='coerce')\n",
    "\n",
    "# Get the full date range from timetable\n",
    "tt_min_date = timetable_df['start_date_dt'].min()\n",
    "tt_max_date = timetable_df['end_date_dt'].max()\n",
    "print(f\"Timetable date range: {tt_min_date.date()} to {tt_max_date.date()}\")\n",
    "\n",
    "# --- Step 3: For each date in the timetable range, count active disruptions ---\n",
    "date_range = pd.date_range(tt_min_date, tt_max_date, freq='D')\n",
    "\n",
    "daily_disruptions = []\n",
    "for single_date in date_range:\n",
    "    # A disruption is active if validity_start <= date and (validity_end is null OR validity_end >= date)\n",
    "    active = disruptions_df[\n",
    "        (disruptions_df['validity_start_dt'] <= pd.Timestamp(single_date, tz='UTC')) &\n",
    "        (\n",
    "            (disruptions_df['validity_end_dt'].isna()) |\n",
    "            (disruptions_df['validity_end_dt'] >= pd.Timestamp(single_date, tz='UTC'))\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Aggregate features\n",
    "    severity_map = {'severe': 3, 'normal': 2, 'slight': 1, 'verySlight': 0, 'unknown': 1, 'noImpact': 0}\n",
    "    severities = active['severity'].map(severity_map).fillna(1)\n",
    "    \n",
    "    reason_counts = active['reason'].value_counts().to_dict()\n",
    "    \n",
    "    daily_disruptions.append({\n",
    "        'date': single_date.date(),\n",
    "        'active_disruptions': len(active),\n",
    "        'unique_situations': active['situation_number'].nunique(),\n",
    "        'avg_severity_score': severities.mean() if len(severities) > 0 else 0,\n",
    "        'max_severity_score': severities.max() if len(severities) > 0 else 0,\n",
    "        'planned_count': (active['planned'] == 'true').sum(),\n",
    "        'unplanned_count': (active['planned'] == 'false').sum(),\n",
    "        'roadworks_count': reason_counts.get('roadworks', 0),\n",
    "        'roadclosed_count': reason_counts.get('roadClosed', 0),\n",
    "        'other_reason_count': sum(v for k, v in reason_counts.items() if k not in ['roadworks', 'roadClosed']),\n",
    "        'regions_affected': ', '.join(active['participant_ref'].unique()),\n",
    "    })\n",
    "\n",
    "daily_disruptions_df = pd.DataFrame(daily_disruptions)\n",
    "print(f\"\\nDaily disruption features: {daily_disruptions_df.shape}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(daily_disruptions_df.head(10).to_string(index=False))\n",
    "print(f\"\\n\\nDisruption count stats:\")\n",
    "print(daily_disruptions_df['active_disruptions'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ba5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED DataFrame shape: (34922, 41)\n",
      "Columns (41):\n",
      "   1. file_name\n",
      "   2. vehicle_journey_code\n",
      "   3. journey_code\n",
      "   4. departure_time\n",
      "   5. service_code\n",
      "   6. line_name\n",
      "   7. direction\n",
      "   8. origin\n",
      "   9. destination\n",
      "  10. start_date\n",
      "  11. end_date\n",
      "  12. days_of_week\n",
      "  13. operator_code\n",
      "  14. operator_name\n",
      "  15. stop_sequence\n",
      "  16. stop_ref\n",
      "  17. stop_name\n",
      "  18. longitude\n",
      "  19. latitude\n",
      "  20. scheduled_arrival\n",
      "  21. wait_time_min\n",
      "  22. run_time_min\n",
      "  23. start_date_dt\n",
      "  24. end_date_dt\n",
      "  25. mid_date\n",
      "  26. date\n",
      "  27. active_disruptions\n",
      "  28. unique_situations\n",
      "  29. avg_severity_score\n",
      "  30. max_severity_score\n",
      "  31. planned_count\n",
      "  32. unplanned_count\n",
      "  33. roadworks_count\n",
      "  34. roadclosed_count\n",
      "  35. other_reason_count\n",
      "  36. regions_affected\n",
      "  37. day_of_week_num\n",
      "  38. departure_hour\n",
      "  39. departure_minute\n",
      "  40. departure_decimal\n",
      "  41. is_peak_hour\n",
      "\n",
      "Null check on disruption columns:\n",
      "  active_disruptions: 0 nulls (0.0%)\n",
      "  avg_severity_score: 0 nulls (0.0%)\n",
      "  planned_count: 0 nulls (0.0%)\n",
      "  roadworks_count: 0 nulls (0.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>vehicle_journey_code</th>\n",
       "      <th>journey_code</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>service_code</th>\n",
       "      <th>line_name</th>\n",
       "      <th>direction</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>unplanned_count</th>\n",
       "      <th>roadworks_count</th>\n",
       "      <th>roadclosed_count</th>\n",
       "      <th>other_reason_count</th>\n",
       "      <th>regions_affected</th>\n",
       "      <th>day_of_week_num</th>\n",
       "      <th>departure_hour</th>\n",
       "      <th>departure_minute</th>\n",
       "      <th>departure_decimal</th>\n",
       "      <th>is_peak_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSB_22E_ENSBPF00019613722_20260105_20260211_2...</td>\n",
       "      <td>vj_1</td>\n",
       "      <td>97</td>\n",
       "      <td>17:43:00</td>\n",
       "      <td>PF0001961:37</td>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>Purfleet-on-Thames Stn/ Aveley, Usk Road</td>\n",
       "      <td>Grays</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name vehicle_journey_code  \\\n",
       "0  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "1  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "2  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "3  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "4  ENSB_22E_ENSBPF00019613722_20260105_20260211_2...                 vj_1   \n",
       "\n",
       "  journey_code departure_time  service_code line_name direction  \\\n",
       "0           97       17:43:00  PF0001961:37        22  outbound   \n",
       "1           97       17:43:00  PF0001961:37        22  outbound   \n",
       "2           97       17:43:00  PF0001961:37        22  outbound   \n",
       "3           97       17:43:00  PF0001961:37        22  outbound   \n",
       "4           97       17:43:00  PF0001961:37        22  outbound   \n",
       "\n",
       "                                     origin destination  start_date  ...  \\\n",
       "0  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "1  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "2  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "3  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "4  Purfleet-on-Thames Stn/ Aveley, Usk Road       Grays  2026-01-05  ...   \n",
       "\n",
       "  unplanned_count roadworks_count roadclosed_count other_reason_count  \\\n",
       "0              22              43               16                 23   \n",
       "1              22              43               16                 23   \n",
       "2              22              43               16                 23   \n",
       "3              22              43               16                 23   \n",
       "4              22              43               16                 23   \n",
       "\n",
       "                                    regions_affected day_of_week_num  \\\n",
       "0  WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...               0   \n",
       "1  WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...               0   \n",
       "2  WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...               0   \n",
       "3  WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...               0   \n",
       "4  WestofEngland, WYCA, TfGM, SYMCA, Merseytravel...               0   \n",
       "\n",
       "  departure_hour  departure_minute  departure_decimal is_peak_hour  \n",
       "0           17.0              43.0          17.716667            1  \n",
       "1           17.0              43.0          17.716667            1  \n",
       "2           17.0              43.0          17.716667            1  \n",
       "3           17.0              43.0          17.716667            1  \n",
       "4           17.0              43.0          17.716667            1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Merge timetable with daily disruption features\n",
    "# ============================================================\n",
    "\n",
    "# For each timetable row, we join the disruption features for every date \n",
    "# in that journey's operating period. We pick the MID-POINT date of\n",
    "# each journey's operating period as the representative date.\n",
    "\n",
    "timetable_df['mid_date'] = timetable_df['start_date_dt'] + \\\n",
    "    (timetable_df['end_date_dt'] - timetable_df['start_date_dt']) / 2\n",
    "timetable_df['mid_date'] = timetable_df['mid_date'].dt.date\n",
    "\n",
    "# Convert daily_disruptions_df date for merge\n",
    "daily_disruptions_df['date'] = pd.to_datetime(daily_disruptions_df['date']).dt.date\n",
    "\n",
    "# Merge on nearest date\n",
    "merged_df = timetable_df.merge(\n",
    "    daily_disruptions_df,\n",
    "    left_on='mid_date',\n",
    "    right_on='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Also add a \"day_of_week_num\" for the departure (useful for prediction)\n",
    "day_map = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "# Extract first day from days_of_week for a numeric feature\n",
    "def get_first_day_num(days_str):\n",
    "    for day, num in day_map.items():\n",
    "        if day in str(days_str):\n",
    "            return num\n",
    "    return -1\n",
    "\n",
    "merged_df['day_of_week_num'] = merged_df['days_of_week'].apply(get_first_day_num)\n",
    "\n",
    "# Parse departure_time to numeric (hours as float)\n",
    "merged_df['departure_hour'] = merged_df['departure_time'].str.split(':').str[0].astype(float)\n",
    "merged_df['departure_minute'] = merged_df['departure_time'].str.split(':').str[1].astype(float)\n",
    "merged_df['departure_decimal'] = merged_df['departure_hour'] + merged_df['departure_minute'] / 60\n",
    "\n",
    "# Add is_peak feature\n",
    "merged_df['is_peak_hour'] = merged_df['departure_hour'].apply(\n",
    "    lambda h: 1 if (7 <= h <= 9) or (16 <= h <= 18) else 0\n",
    ")\n",
    "\n",
    "print(f\"MERGED DataFrame shape: {merged_df.shape}\")\n",
    "print(f\"Columns ({len(merged_df.columns)}):\")\n",
    "for i, col in enumerate(merged_df.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nNull check on disruption columns:\")\n",
    "disrupt_cols = ['active_disruptions', 'avg_severity_score', 'planned_count', 'roadworks_count']\n",
    "for col in disrupt_cols:\n",
    "    nulls = merged_df[col].isna().sum()\n",
    "    print(f\"  {col}: {nulls} nulls ({nulls/len(merged_df)*100:.1f}%)\")\n",
    "\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801e42f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ensign_timetable_with_disruptions.csv (34922 rows x 36 cols)\n",
      "Saved: daily_disruption_features.csv (448 rows x 11 cols)\n",
      "\n",
      "All files in F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed:\n",
      "  daily_disruption_features.csv: 38.9 KB\n",
      "  ensign_bus_journeys.csv: 839.5 KB\n",
      "  ensign_bus_stops.csv: 22.7 KB\n",
      "  ensign_bus_timetable.csv: 8648.1 KB\n",
      "  ensign_timetable_with_disruptions.csv: 11804.2 KB\n",
      "  sirisx_disruptions.csv: 376.5 KB\n",
      "\n",
      "============================================================\n",
      "MERGE SUMMARY\n",
      "============================================================\n",
      "  Direct merge keys (operator, stop refs): NO OVERLAP\n",
      "  Geographic overlap: NO (disruptions are in NW/SW England)\n",
      "  TEMPORAL merge: YES - joined daily disruption counts to timetable dates\n",
      "  Result: 34,922 rows x 36 columns\n",
      "  Disruption features added: 10 columns (counts, severity, reasons)\n",
      "  This is valid for 'national disruption climate' affecting network performance\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Save merged dataset + clean up columns for analysis\n",
    "# ============================================================\n",
    "\n",
    "# Select and order columns for the final merged CSV\n",
    "final_cols = [\n",
    "    # Journey identifiers\n",
    "    'vehicle_journey_code', 'journey_code', 'line_name', 'direction',\n",
    "    'service_code', 'operator_code', 'operator_name',\n",
    "    # Route info\n",
    "    'origin', 'destination', 'stop_sequence', 'stop_ref', 'stop_name',\n",
    "    'longitude', 'latitude',\n",
    "    # Schedule info\n",
    "    'departure_time', 'scheduled_arrival', 'start_date', 'end_date',\n",
    "    'days_of_week',\n",
    "    # Timing features\n",
    "    'wait_time_min', 'run_time_min',\n",
    "    # Engineered features\n",
    "    'departure_hour', 'departure_decimal', 'is_peak_hour', 'day_of_week_num',\n",
    "    'mid_date',\n",
    "    # Disruption context features (from SIRI-SX temporal merge)\n",
    "    'active_disruptions', 'unique_situations', 'avg_severity_score',\n",
    "    'max_severity_score', 'planned_count', 'unplanned_count',\n",
    "    'roadworks_count', 'roadclosed_count', 'other_reason_count',\n",
    "    'regions_affected',\n",
    "]\n",
    "\n",
    "final_df = merged_df[final_cols].copy()\n",
    "\n",
    "# Save to data/processed\n",
    "output_dir = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Main merged dataset\n",
    "final_df.to_csv(os.path.join(output_dir, 'ensign_timetable_with_disruptions.csv'), index=False)\n",
    "print(f\"Saved: ensign_timetable_with_disruptions.csv ({final_df.shape[0]} rows x {final_df.shape[1]} cols)\")\n",
    "\n",
    "# Also save the daily disruption features separately\n",
    "daily_disruptions_df.to_csv(os.path.join(output_dir, 'daily_disruption_features.csv'), index=False)\n",
    "print(f\"Saved: daily_disruption_features.csv ({daily_disruptions_df.shape[0]} rows x {daily_disruptions_df.shape[1]} cols)\")\n",
    "\n",
    "# Keep the separate datasets too\n",
    "timetable_df.drop(columns=['start_date_dt', 'end_date_dt', 'mid_date'], errors='ignore').to_csv(\n",
    "    os.path.join(output_dir, 'ensign_bus_timetable.csv'), index=False)\n",
    "stops_df = pd.DataFrame(all_stops.values())\n",
    "stops_df.to_csv(os.path.join(output_dir, 'ensign_bus_stops.csv'), index=False)\n",
    "disruptions_df.to_csv(os.path.join(output_dir, 'sirisx_disruptions.csv'), index=False)\n",
    "journeys_df = pd.DataFrame(all_journeys)\n",
    "journeys_df.to_csv(os.path.join(output_dir, 'ensign_bus_journeys.csv'), index=False)\n",
    "\n",
    "print(f\"\\nAll files in {output_dir}:\")\n",
    "for f in sorted(os.listdir(output_dir)):\n",
    "    if f.endswith('.csv'):\n",
    "        size = os.path.getsize(os.path.join(output_dir, f))\n",
    "        print(f\"  {f}: {size/1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MERGE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Direct merge keys (operator, stop refs): NO OVERLAP\")\n",
    "print(f\"  Geographic overlap: NO (disruptions are in NW/SW England)\")\n",
    "print(f\"  TEMPORAL merge: YES - joined daily disruption counts to timetable dates\")\n",
    "print(f\"  Result: {final_df.shape[0]:,} rows x {final_df.shape[1]} columns\")\n",
    "print(f\"  Disruption features added: 10 columns (counts, severity, reasons)\")\n",
    "print(f\"  This is valid for 'national disruption climate' affecting network performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
