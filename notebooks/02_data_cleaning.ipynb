{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6f2950",
   "metadata": {},
   "source": [
    "# 02 - Data Cleaning & Preprocessing\n",
    "\n",
    "**Objective**: Load the merged timetable + disruption dataset, inspect data quality, handle missing values, remove duplicates, fix data types, and produce a clean dataset for feature engineering.\n",
    "\n",
    "**Input**: `data/processed/ensign_timetable_with_disruptions.csv`  \n",
    "**Output**: `data/processed/cleaned_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports & Spark Session\n",
    "# ============================================================\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session ready!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2733009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Raw Merged Dataset\n",
    "# ============================================================\n",
    "DATA_PATH = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed\\ensign_timetable_with_disruptions.csv'\n",
    "\n",
    "pdf = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset loaded: {pdf.shape[0]:,} rows x {pdf.shape[1]} columns\")\n",
    "print(f\"\\nAll columns ({pdf.shape[1]}):\")\n",
    "for i, col in enumerate(pdf.columns, 1):\n",
    "    print(f\"  {i:2d}. {col:35s} dtype={pdf[col].dtype}\")\n",
    "\n",
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd031f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Data Quality Inspection\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "null_counts = pdf.isnull().sum()\n",
    "null_pct = (pdf.isnull().sum() / len(pdf) * 100).round(2)\n",
    "null_report = pd.DataFrame({'nulls': null_counts, 'pct': null_pct})\n",
    "null_report = null_report[null_report['nulls'] > 0]\n",
    "if len(null_report) > 0:\n",
    "    print(null_report.to_string())\n",
    "else:\n",
    "    print(\"   No missing values found!\")\n",
    "\n",
    "# Duplicates\n",
    "print(f\"\\n2. DUPLICATE ROWS:\")\n",
    "n_dupes = pdf.duplicated().sum()\n",
    "print(f\"   Full row duplicates: {n_dupes} ({n_dupes/len(pdf)*100:.2f}%)\")\n",
    "\n",
    "# Data type issues\n",
    "print(f\"\\n3. DATA TYPES:\")\n",
    "print(f\"   Numeric columns:  {pdf.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"   Object columns:   {pdf.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "# Value ranges for key numeric columns\n",
    "print(f\"\\n4. VALUE RANGES (numeric):\")\n",
    "num_cols = ['departure_hour', 'stop_sequence', 'latitude', 'longitude',\n",
    "            'active_disruptions', 'avg_severity_score', 'run_time_min',\n",
    "            'day_of_week_num', 'is_peak_hour']\n",
    "for col in num_cols:\n",
    "    if col in pdf.columns:\n",
    "        print(f\"   {col:25s} min={pdf[col].min():>10.2f}  max={pdf[col].max():>10.2f}  mean={pdf[col].mean():>10.2f}\")\n",
    "\n",
    "# Unique values for categorical columns\n",
    "print(f\"\\n5. CATEGORICAL COLUMNS:\")\n",
    "cat_cols = ['line_name', 'direction', 'operator_code', 'days_of_week']\n",
    "for col in cat_cols:\n",
    "    if col in pdf.columns:\n",
    "        print(f\"   {col:25s} unique={pdf[col].nunique():>5}  values={list(pdf[col].unique()[:8])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Handle Missing Values\n",
    "# ============================================================\n",
    "\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "before_rows = len(pdf)\n",
    "\n",
    "# Fill numeric nulls with 0 (disruption counts) or median (continuous)\n",
    "disruption_cols = ['active_disruptions', 'unique_situations', 'avg_severity_score',\n",
    "                   'max_severity_score', 'planned_count', 'unplanned_count',\n",
    "                   'roadworks_count', 'roadclosed_count', 'other_reason_count']\n",
    "for col in disruption_cols:\n",
    "    if col in pdf.columns:\n",
    "        nulls_before = pdf[col].isnull().sum()\n",
    "        if nulls_before > 0:\n",
    "            pdf[col] = pdf[col].fillna(0)\n",
    "            print(f\"  {col}: filled {nulls_before} nulls with 0\")\n",
    "\n",
    "# Fill remaining numeric with median\n",
    "for col in pdf.select_dtypes(include=[np.number]).columns:\n",
    "    nulls_before = pdf[col].isnull().sum()\n",
    "    if nulls_before > 0:\n",
    "        median_val = pdf[col].median()\n",
    "        pdf[col] = pdf[col].fillna(median_val)\n",
    "        print(f\"  {col}: filled {nulls_before} nulls with median ({median_val:.2f})\")\n",
    "\n",
    "# Fill categorical nulls with 'unknown'\n",
    "for col in pdf.select_dtypes(include=['object']).columns:\n",
    "    nulls_before = pdf[col].isnull().sum()\n",
    "    if nulls_before > 0:\n",
    "        pdf[col] = pdf[col].fillna('unknown')\n",
    "        print(f\"  {col}: filled {nulls_before} nulls with 'unknown'\")\n",
    "\n",
    "remaining_nulls = pdf.isnull().sum().sum()\n",
    "print(f\"\\nRemaining nulls after filling: {remaining_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Remove Duplicates\n",
    "# ============================================================\n",
    "\n",
    "print(\"REMOVING DUPLICATES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "before = len(pdf)\n",
    "pdf = pdf.drop_duplicates()\n",
    "after = len(pdf)\n",
    "dropped = before - after\n",
    "\n",
    "print(f\"  Before: {before:,} rows\")\n",
    "print(f\"  After:  {after:,} rows\")\n",
    "print(f\"  Dropped: {dropped:,} duplicate rows ({dropped/before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f19fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Fix Data Types & Validate Ranges\n",
    "# ============================================================\n",
    "\n",
    "print(\"DATA TYPE CORRECTIONS & RANGE VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ensure correct numeric types\n",
    "pdf['departure_hour'] = pdf['departure_hour'].astype(float)\n",
    "pdf['is_peak_hour'] = pdf['is_peak_hour'].astype(int)\n",
    "pdf['day_of_week_num'] = pdf['day_of_week_num'].astype(int)\n",
    "pdf['stop_sequence'] = pdf['stop_sequence'].astype(int)\n",
    "print(\"  Numeric types enforced.\")\n",
    "\n",
    "# Validate departure_hour is 0-23\n",
    "invalid_hours = pdf[(pdf['departure_hour'] < 0) | (pdf['departure_hour'] > 23)]\n",
    "if len(invalid_hours) > 0:\n",
    "    print(f\"  WARNING: {len(invalid_hours)} rows with invalid departure_hour -> clipping to [0,23]\")\n",
    "    pdf['departure_hour'] = pdf['departure_hour'].clip(0, 23)\n",
    "else:\n",
    "    print(\"  departure_hour range valid (0-23).\")\n",
    "\n",
    "# Validate latitude/longitude (UK should be lat ~50-56, lon ~-6 to 2)\n",
    "invalid_coords = pdf[(pdf['latitude'] < 49) | (pdf['latitude'] > 61) |\n",
    "                     (pdf['longitude'] < -8) | (pdf['longitude'] > 3)]\n",
    "if len(invalid_coords) > 0:\n",
    "    print(f\"  WARNING: {len(invalid_coords)} rows with coordinates outside UK bounds\")\n",
    "else:\n",
    "    print(\"  Coordinates valid (UK bounds).\")\n",
    "\n",
    "# Validate disruption counts are non-negative\n",
    "for col in ['active_disruptions', 'planned_count', 'unplanned_count', 'roadworks_count']:\n",
    "    neg = (pdf[col] < 0).sum()\n",
    "    if neg > 0:\n",
    "        print(f\"  WARNING: {neg} negative values in {col} -> setting to 0\")\n",
    "        pdf[col] = pdf[col].clip(lower=0)\n",
    "\n",
    "print(\"  All disruption counts non-negative.\")\n",
    "\n",
    "# Validate is_peak_hour is binary\n",
    "assert pdf['is_peak_hour'].isin([0, 1]).all(), \"is_peak_hour should be 0 or 1\"\n",
    "print(\"  is_peak_hour confirmed binary (0/1).\")\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset: {pdf.shape[0]:,} rows x {pdf.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866eb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Drop Unnecessary Columns for ML\n",
    "# ============================================================\n",
    "\n",
    "# Columns to DROP: IDs, redundant text, high cardinality, leakage risk\n",
    "DROP_COLS = [\n",
    "    'vehicle_journey_code',  # ID only\n",
    "    'journey_code',          # ID only\n",
    "    'service_code',          # redundant with line_name\n",
    "    'operator_code',         # single value (ENSB)\n",
    "    'operator_name',         # single value (Ensignbus)\n",
    "    'stop_ref',              # ID only\n",
    "    'stop_name',             # high cardinality text\n",
    "    'origin',                # high cardinality text\n",
    "    'destination',           # high cardinality text\n",
    "    'departure_time',        # already encoded as departure_hour\n",
    "    'scheduled_arrival',     # redundant with departure + sequence\n",
    "    'departure_decimal',     # duplicate of departure_hour\n",
    "    'start_date',            # date info captured in mid_date\n",
    "    'end_date',              # date info captured in mid_date\n",
    "    'days_of_week',          # encoded as day_of_week_num\n",
    "    'mid_date',              # temporal leakage risk\n",
    "    'regions_affected',      # string - not useful for ML directly\n",
    "    'max_severity_score',    # highly correlated with avg_severity_score\n",
    "    'other_reason_count',    # low signal, noisy\n",
    "]\n",
    "\n",
    "# Only drop columns that actually exist\n",
    "cols_to_drop = [c for c in DROP_COLS if c in pdf.columns]\n",
    "pdf_clean = pdf.drop(columns=cols_to_drop)\n",
    "\n",
    "print(\"COLUMNS DROPPED (not useful for ML):\")\n",
    "for col in cols_to_drop:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nCOLUMNS KEPT ({pdf_clean.shape[1]}):\")\n",
    "for i, col in enumerate(pdf_clean.columns, 1):\n",
    "    print(f\"  {i:2d}. {col:30s} dtype={pdf_clean[col].dtype}\")\n",
    "\n",
    "print(f\"\\nCleaned dataset: {pdf_clean.shape[0]:,} rows x {pdf_clean.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Save Cleaned Dataset\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "output_dir = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'cleaned_dataset.csv')\n",
    "pdf_clean.to_csv(output_path, index=False)\n",
    "\n",
    "file_size = os.path.getsize(output_path)\n",
    "print(f\"Saved: cleaned_dataset.csv\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "print(f\"  Size: {file_size/1024:.1f} KB ({file_size/1024/1024:.2f} MB)\")\n",
    "print(f\"  Rows: {pdf_clean.shape[0]:,}\")\n",
    "print(f\"  Cols: {pdf_clean.shape[1]}\")\n",
    "\n",
    "print(f\"\\n--- Cleaning Summary ---\")\n",
    "print(f\"  Input:   {DATA_PATH.split(chr(92))[-1]} ({pdf.shape[0]:,} x {pdf.shape[1]})\")\n",
    "print(f\"  Output:  cleaned_dataset.csv ({pdf_clean.shape[0]:,} x {pdf_clean.shape[1]})\")\n",
    "print(f\"  Nulls filled: disruption cols -> 0, numeric -> median, categorical -> 'unknown'\")\n",
    "print(f\"  Duplicates removed: {before - after:,}\")\n",
    "print(f\"  Columns dropped: {len(cols_to_drop)} (IDs, text, redundant, leakage)\")\n",
    "print(f\"\\n  Ready for 03_feature_engineering.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
