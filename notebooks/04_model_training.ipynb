{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c3262e",
   "metadata": {},
   "source": [
    "# 04 - Model Training\n",
    "\n",
    "**Objective**: Train two classification models to predict disruption impact risk.\n",
    "\n",
    "- **Model 1 (Baseline)**: Logistic Regression — simple, interpretable, linear\n",
    "- **Model 2 (Main)**: Random Forest Classifier — non-linear, ensemble, robust\n",
    "\n",
    "**Input**: `data/processed/train_prepared.parquet`, `data/processed/test_prepared.parquet`  \n",
    "**Output**: Trained models saved to `data/processed/`, predictions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports & Spark Session\n",
    "# ============================================================\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ModelTraining\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "DATA_DIR = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed'\n",
    "MODEL_DIR = os.path.join(DATA_DIR, 'models')\n",
    "OUTPUT_DIR = r'F:\\SOFTWARICA\\big-data-transport-analytics\\outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20187075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: high_disruption_risk\n",
      "Threshold: 107.0\n",
      "Features: 11 numeric + 4 categorical\n",
      "\n",
      "Train set: 28,119 rows\n",
      "Test set:  6,803 rows\n",
      "Feature vector size: 35\n",
      "\n",
      "Train class balance:\n",
      "+--------------------+-----+\n",
      "|high_disruption_risk|count|\n",
      "+--------------------+-----+\n",
      "|                   0|21550|\n",
      "|                   1| 6569|\n",
      "+--------------------+-----+\n",
      "\n",
      "Test class balance:\n",
      "+--------------------+-----+\n",
      "|high_disruption_risk|count|\n",
      "+--------------------+-----+\n",
      "|                   0| 5224|\n",
      "|                   1| 1579|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Data, Rebuild Pipeline & Prepare Features\n",
    "# ============================================================\n",
    "\n",
    "# Load metadata from feature engineering\n",
    "with open(os.path.join(MODEL_DIR, 'feature_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "TARGET = metadata['target']\n",
    "THRESHOLD = metadata['threshold']\n",
    "NUM_FEATURES = metadata['num_features']\n",
    "CAT_FEATURES = metadata['cat_features']\n",
    "ALL_FEATURES = CAT_FEATURES + NUM_FEATURES\n",
    "\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(f\"Features: {len(NUM_FEATURES)} numeric + {len(CAT_FEATURES)} categorical\")\n",
    "\n",
    "# Load train/test CSVs into Spark\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "train_pdf = pd.read_csv(os.path.join(DATA_DIR, 'train_split.csv'))\n",
    "test_pdf = pd.read_csv(os.path.join(DATA_DIR, 'test_split.csv'))\n",
    "\n",
    "train_df = spark.createDataFrame(train_pdf[ALL_FEATURES + [TARGET]])\n",
    "test_df = spark.createDataFrame(test_pdf[ALL_FEATURES + [TARGET]])\n",
    "\n",
    "print(f\"\\nTrain set: {train_df.count():,} rows\")\n",
    "print(f\"Test set:  {test_df.count():,} rows\")\n",
    "\n",
    "# Rebuild preprocessing pipeline (same as notebook 03)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f'{col}_idx', handleInvalid='keep')\n",
    "    for col in CAT_FEATURES\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f'{col}_idx', outputCol=f'{col}_vec')\n",
    "    for col in CAT_FEATURES\n",
    "]\n",
    "encoded_cols = [f'{col}_vec' for col in CAT_FEATURES]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=NUM_FEATURES + encoded_cols,\n",
    "    outputCol='features_raw'\n",
    ")\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withStd=True, withMean=False)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit on training data, transform both\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "train_prepared = pipeline_model.transform(train_df)\n",
    "test_prepared = pipeline_model.transform(test_df)\n",
    "\n",
    "FEATURE_SIZE = train_prepared.select('features').first()[0].size\n",
    "print(f\"Feature vector size: {FEATURE_SIZE}\")\n",
    "\n",
    "print(f\"\\nTrain class balance:\")\n",
    "train_prepared.groupBy(TARGET).count().orderBy(TARGET).show()\n",
    "\n",
    "print(f\"Test class balance:\")\n",
    "test_prepared.groupBy(TARGET).count().orderBy(TARGET).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112dbe",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 1: Logistic Regression (Baseline)\n",
    "\n",
    "**Why?** Simple, fast, interpretable. A linear model that serves as the baseline to beat. If Random Forest can't outperform this, then the data may not have non-linear patterns worth capturing.\n",
    "\n",
    "**Complexity**: Training O(n * p * iterations), Prediction O(p) per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f135c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 1: LOGISTIC REGRESSION (Baseline)\n",
      "============================================================\n",
      "Hyperparameters:\n",
      "  maxIter:         100\n",
      "  regParam (L2):   0.01\n",
      "  elasticNet:      0.0 (0.0 = pure L2)\n",
      "  threshold:       0.5\n",
      "\n",
      "Training completed in 35.41 seconds\n",
      "\n",
      "--- Training Metrics ---\n",
      "  Training Accuracy: 0.8507\n",
      "  Training AUC-ROC:  0.8866\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Train Logistic Regression\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (Baseline)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol=TARGET,\n",
    "    maxIter=100,\n",
    "    regParam=0.01,         # L2 regularization\n",
    "    elasticNetParam=0.0,   # Pure L2 (Ridge)\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"  maxIter:         {lr.getMaxIter()}\")\n",
    "print(f\"  regParam (L2):   {lr.getRegParam()}\")\n",
    "print(f\"  elasticNet:      {lr.getElasticNetParam()} (0.0 = pure L2)\")\n",
    "print(f\"  threshold:       {lr.getThreshold()}\")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "lr_model = lr.fit(train_prepared)\n",
    "lr_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {lr_train_time:.2f} seconds\")\n",
    "\n",
    "# Training summary\n",
    "lr_summary = lr_model.summary\n",
    "print(f\"\\n--- Training Metrics ---\")\n",
    "print(f\"  Training Accuracy: {lr_summary.accuracy:.4f}\")\n",
    "print(f\"  Training AUC-ROC:  {lr_summary.areaUnderROC:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10598e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOGISTIC REGRESSION - Test Set Results\n",
      "============================================================\n",
      "  accuracy            : 0.8540\n",
      "  f1                  : 0.8301\n",
      "  precision           : 0.8670\n",
      "  recall              : 0.8540\n",
      "  auc_roc             : 0.8906\n",
      "  train_time          : 35.4102\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "+--------------------+----------+-----+\n",
      "|high_disruption_risk|prediction|count|\n",
      "+--------------------+----------+-----+\n",
      "|                   0|       0.0| 5185|\n",
      "|                   0|       1.0|   39|\n",
      "|                   1|       0.0|  954|\n",
      "|                   1|       1.0|  625|\n",
      "+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Logistic Regression - Test Predictions\n",
    "# ============================================================\n",
    "\n",
    "# Predict on test set\n",
    "lr_predictions = lr_model.transform(test_prepared)\n",
    "\n",
    "# Evaluate with multiple metrics\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=TARGET, metricName='areaUnderROC')\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol=TARGET, metricName='accuracy')\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=TARGET, metricName='f1')\n",
    "prec_eval = MulticlassClassificationEvaluator(labelCol=TARGET, metricName='weightedPrecision')\n",
    "rec_eval = MulticlassClassificationEvaluator(labelCol=TARGET, metricName='weightedRecall')\n",
    "\n",
    "lr_metrics = {\n",
    "    'accuracy': acc_eval.evaluate(lr_predictions),\n",
    "    'f1': f1_eval.evaluate(lr_predictions),\n",
    "    'precision': prec_eval.evaluate(lr_predictions),\n",
    "    'recall': rec_eval.evaluate(lr_predictions),\n",
    "    'auc_roc': binary_eval.evaluate(lr_predictions),\n",
    "    'train_time': lr_train_time,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION - Test Set Results\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Test):\")\n",
    "lr_predictions.groupBy(TARGET, 'prediction').count().orderBy(TARGET, 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bd0c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 2: Random Forest Classifier (Main Model)\n",
    "\n",
    "**Why?** Handles non-linear relationships, mixed feature types, robust to outliers. Ensemble of 100 decision trees — majority vote for classification.\n",
    "\n",
    "**Complexity**: Training O(T * n * p * log(n)), Prediction O(T * depth) per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c21769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 2: RANDOM FOREST CLASSIFIER (Main Model)\n",
      "============================================================\n",
      "Hyperparameters:\n",
      "  numTrees:              100\n",
      "  maxDepth:              10\n",
      "  minInstancesPerNode:   5\n",
      "  featureSubsetStrategy: sqrt\n",
      "  seed:                  42\n",
      "\n",
      "Training completed in 52.24 seconds\n",
      "Number of trees: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Train Random Forest\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL 2: RANDOM FOREST CLASSIFIER (Main Model)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol=TARGET,\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    featureSubsetStrategy='sqrt',\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"  numTrees:              {rf.getNumTrees()}\")\n",
    "print(f\"  maxDepth:              {rf.getMaxDepth()}\")\n",
    "print(f\"  minInstancesPerNode:   {rf.getMinInstancesPerNode()}\")\n",
    "print(f\"  featureSubsetStrategy: {rf.getFeatureSubsetStrategy()}\")\n",
    "print(f\"  seed:                  {rf.getSeed()}\")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "rf_model = rf.fit(train_prepared)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {rf_train_time:.2f} seconds\")\n",
    "print(f\"Number of trees: {rf_model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cac1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST - Test Set Results\n",
      "============================================================\n",
      "  accuracy            : 0.8584\n",
      "  f1                  : 0.8380\n",
      "  precision           : 0.8666\n",
      "  recall              : 0.8584\n",
      "  auc_roc             : 0.8795\n",
      "  train_time          : 52.2450\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "+--------------------+----------+-----+\n",
      "|high_disruption_risk|prediction|count|\n",
      "+--------------------+----------+-----+\n",
      "|                   0|       0.0| 5163|\n",
      "|                   0|       1.0|   61|\n",
      "|                   1|       0.0|  902|\n",
      "|                   1|       1.0|  677|\n",
      "+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Random Forest - Test Predictions\n",
    "# ============================================================\n",
    "\n",
    "# Predict on test set\n",
    "rf_predictions = rf_model.transform(test_prepared)\n",
    "\n",
    "rf_metrics = {\n",
    "    'accuracy': acc_eval.evaluate(rf_predictions),\n",
    "    'f1': f1_eval.evaluate(rf_predictions),\n",
    "    'precision': prec_eval.evaluate(rf_predictions),\n",
    "    'recall': rec_eval.evaluate(rf_predictions),\n",
    "    'auc_roc': binary_eval.evaluate(rf_predictions),\n",
    "    'train_time': rf_train_time,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST - Test Set Results\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Test):\")\n",
    "rf_predictions.groupBy(TARGET, 'prediction').count().orderBy(TARGET, 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687a39c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST - Feature Importance Ranking\n",
      "=======================================================\n",
      "   1. day_of_week_num           0.4445 ████████████████████████████████████████████\n",
      "   2. is_weekend                0.3644 ████████████████████████████████████\n",
      "   3. line_name_88              0.0469 ████\n",
      "   4. line_name_x32             0.0398 ███\n",
      "   5. latitude                  0.0118 █\n",
      "   6. stop_sequence             0.0103 █\n",
      "   7. line_name_22              0.0101 █\n",
      "   8. departure_hour            0.0099 \n",
      "   9. longitude                 0.0069 \n",
      "  10. hour_cos                  0.0065 \n",
      "  11. line_name_33              0.0056 \n",
      "  12. hour_sin                  0.0053 \n",
      "  13. route_complexity          0.0051 \n",
      "  14. line_name_99OT            0.0036 \n",
      "  15. run_time_min              0.0034 \n",
      "  16. line_name_73              0.0032 \n",
      "  17. time_of_day_midday        0.0032 \n",
      "  18. direction_inbound         0.0030 \n",
      "  19. line_name_x80             0.0015 \n",
      "  20. line_name_83              0.0015 \n",
      "  21. lat_zone_mid_north        0.0012 \n",
      "  22. line_name_x2              0.0010 \n",
      "  23. line_name_44              0.0010 \n",
      "  24. lat_zone_south            0.0009 \n",
      "  25. time_of_day_late_night    0.0008 \n",
      "  26. time_of_day_evening_rush  0.0008 \n",
      "  27. time_of_day_morning_rush  0.0006 \n",
      "  28. line_name_99              0.0005 \n",
      "  29. is_peak_hour              0.0005 \n",
      "  30. lat_zone_mid_south        0.0004 \n",
      "  31. direction_outbound        0.0004 \n",
      "\n",
      "Saved: outputs/feature_importance.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_31448\\2977761481.py:49: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Feature Importance (Random Forest)\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Build feature names in same order as VectorAssembler\n",
    "# First: numeric features\n",
    "feature_names = metadata['num_features'].copy()\n",
    "\n",
    "# Then: one-hot encoded categorical features (N-1 for each)\n",
    "cat_labels = metadata['cat_labels']\n",
    "for col in metadata['cat_features']:\n",
    "    labels = cat_labels[col]\n",
    "    for label in labels[:-1]:  # N-1 (last is reference category)\n",
    "        feature_names.append(f'{col}_{label}')\n",
    "\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "n = min(len(feature_names), len(importances))\n",
    "feat_imp = sorted(zip(feature_names[:n], importances[:n]),\n",
    "                  key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"RANDOM FOREST - Feature Importance Ranking\")\n",
    "print(\"=\" * 55)\n",
    "for i, (name, imp) in enumerate(feat_imp, 1):\n",
    "    bar = '█' * int(imp * 100)\n",
    "    print(f\"  {i:2d}. {name:25s} {imp:.4f} {bar}\")\n",
    "\n",
    "# Plot top features\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "top_n = min(15, len(feat_imp))\n",
    "names = [x[0] for x in feat_imp[:top_n]]\n",
    "vals = [x[1] for x in feat_imp[:top_n]]\n",
    "colors = ['#e74c3c' if v > 0.05 else '#3498db' for v in vals]\n",
    "\n",
    "ax.barh(range(len(names)), vals, color=colors)\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Random Forest - Top Feature Importances', fontweight='bold', fontsize=13)\n",
    "for i, v in enumerate(vals):\n",
    "    ax.text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nSaved: outputs/feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Metric                   Logistic Reg  Random Forest           Winner\n",
      "----------------------------------------------------------------------\n",
      "  accuracy                     0.8540         0.8584    Random Forest\n",
      "  f1                           0.8301         0.8380    Random Forest\n",
      "  precision                    0.8670         0.8666     Logistic Reg\n",
      "  recall                       0.8540         0.8584    Random Forest\n",
      "  auc_roc                      0.8906         0.8795     Logistic Reg\n",
      "  train_time                  35.4102        52.2450      LR (faster)\n",
      "\n",
      "Saved: data/processed/lr_predictions.csv (6,803 rows)\n",
      "Saved: data/processed/rf_predictions.csv (6,803 rows)\n",
      "\n",
      "Attempting to save PySpark models...\n",
      "  ✗ PySpark model save failed (expected on Windows): Py4JJavaError\n",
      "    Falling back to parameter export...\n",
      "\n",
      "Exporting model parameters for GUI...\n",
      "  ✓ Saved: data/processed/lr_model_params.pkl\n",
      "  ✓ Saved: data/processed/rf_model_params.pkl\n",
      "\n",
      "  For GUI inference:\n",
      "    - Load params with: pickle.load(open('rf_model_params.pkl', 'rb'))\n",
      "    - Or reload full PySpark model if available\n",
      "    - Pipeline metadata in: feature_metadata.json\n",
      "\n",
      "Saved: data/processed/model_metrics.json\n",
      "\n",
      "--- Training Complete ---\n",
      "  Ready for 05_evaluation.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Quick Comparison & Save Model Outputs\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<22} {'Logistic Reg':>14} {'Random Forest':>14} {'Winner':>16}\")\n",
    "print(\"-\" * 70)\n",
    "for metric in ['accuracy', 'f1', 'precision', 'recall', 'auc_roc', 'train_time']:\n",
    "    lr_val = lr_metrics[metric]\n",
    "    rf_val = rf_metrics[metric]\n",
    "    if metric == 'train_time':\n",
    "        winner = 'LR (faster)' if lr_val < rf_val else 'RF (faster)'\n",
    "    else:\n",
    "        winner = 'Random Forest' if rf_val > lr_val else ('Logistic Reg' if lr_val > rf_val else 'Tie')\n",
    "    print(f\"  {metric:<20} {lr_val:>14.4f} {rf_val:>14.4f} {winner:>16}\")\n",
    "\n",
    "# --- Save predictions as CSV via pandas (avoids Hadoop/winutils) ---\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "get_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "\n",
    "# Extract label, prediction, probability(class=1) to pandas and save\n",
    "lr_out = lr_predictions.withColumn('prob_1', get_prob('probability')) \\\n",
    "    .select(TARGET, 'prediction', 'prob_1').toPandas()\n",
    "lr_out.to_csv(os.path.join(DATA_DIR, 'lr_predictions.csv'), index=False)\n",
    "print(f\"\\nSaved: data/processed/lr_predictions.csv ({len(lr_out):,} rows)\")\n",
    "\n",
    "rf_out = rf_predictions.withColumn('prob_1', get_prob('probability')) \\\n",
    "    .select(TARGET, 'prediction', 'prob_1').toPandas()\n",
    "rf_out.to_csv(os.path.join(DATA_DIR, 'rf_predictions.csv'), index=False)\n",
    "print(f\"Saved: data/processed/rf_predictions.csv ({len(rf_out):,} rows)\")\n",
    "\n",
    "# --- Save trained models (for GUI reuse) ---\n",
    "import pickle\n",
    "\n",
    "# Try saving full PySpark models (may fail on Windows without Hadoop)\n",
    "print(f\"\\nAttempting to save PySpark models...\")\n",
    "try:\n",
    "    lr_model.write().overwrite().save(os.path.join(MODEL_DIR, 'lr_model_spark'))\n",
    "    rf_model.write().overwrite().save(os.path.join(MODEL_DIR, 'rf_model_spark'))\n",
    "    print(f\"  ✓ Saved PySpark native models to data/processed/models/\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ PySpark model save failed (expected on Windows): {type(e).__name__}\")\n",
    "    print(f\"    Falling back to parameter export...\")\n",
    "\n",
    "# Extract and save model parameters (always works, GUI-friendly)\n",
    "print(f\"\\nExporting model parameters for GUI...\")\n",
    "\n",
    "# Logistic Regression parameters\n",
    "lr_params = {\n",
    "    'type': 'LogisticRegression',\n",
    "    'coefficients': lr_model.coefficients.toArray().tolist(),\n",
    "    'intercept': float(lr_model.intercept),\n",
    "    'num_features': lr_model.numFeatures,\n",
    "    'num_classes': lr_model.numClasses,\n",
    "    'feature_names': metadata['num_features'] + \\\n",
    "                     [f'{col}_{lbl}' for col in metadata['cat_features'] \n",
    "                      for lbl in metadata['cat_labels'][col][:-1]],\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, 'lr_model_params.pkl'), 'wb') as f:\n",
    "    pickle.dump(lr_params, f)\n",
    "print(f\"  ✓ Saved: data/processed/models/lr_model_params.pkl\")\n",
    "\n",
    "# Random Forest parameters + feature importances\n",
    "rf_params = {\n",
    "    'type': 'RandomForestClassifier',\n",
    "    'num_trees': rf_model.getNumTrees,\n",
    "    'feature_importances': rf_model.featureImportances.toArray().tolist(),\n",
    "    'num_features': rf_model.numFeatures,\n",
    "    'feature_names': lr_params['feature_names'],  # same as LR\n",
    "    'tree_weights': rf_model.treeWeights,\n",
    "    # Note: Full tree structures not exported (use PySpark model for inference)\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, 'rf_model_params.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_params, f)\n",
    "print(f\"  ✓ Saved: data/processed/models/rf_model_params.pkl\")\n",
    "\n",
    "print(f\"\\n  For GUI inference:\")\n",
    "print(f\"    - Load params with: pickle.load(open('rf_model_params.pkl', 'rb'))\")\n",
    "print(f\"    - Or reload full PySpark model if available\")\n",
    "print(f\"    - Pipeline metadata in: feature_metadata.json\")\n",
    "\n",
    "# Save metrics JSON\n",
    "all_metrics = {'logistic_regression': lr_metrics, 'random_forest': rf_metrics}\n",
    "with open(os.path.join(MODEL_DIR, 'model_metrics.json'), 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"\\nSaved: data/processed/models/model_metrics.json\")\n",
    "\n",
    "print(f\"\\n--- Training Complete ---\")\n",
    "print(f\"  Ready for 05_evaluation.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599fa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
