{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb495d6c",
   "metadata": {},
   "source": [
    "# 03 - Feature Engineering\n",
    "\n",
    "**Objective**: Create the target variable, engineer features, encode categoricals, scale numerics, and prepare train/test splits ready for model training.\n",
    "\n",
    "**Input**: `data/processed/cleaned_dataset.csv`  \n",
    "**Output**: `data/processed/featured_dataset.csv` + PySpark prepared train/test DataFrames saved as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ea027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports & Spark Session\n",
    "# ============================================================\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeatureEngineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "DATA_DIR = r'F:\\SOFTWARICA\\big-data-transport-analytics\\data\\processed'\n",
    "MODEL_DIR = os.path.join(DATA_DIR, 'models')\n",
    "OUTPUT_DIR = r'F:\\SOFTWARICA\\big-data-transport-analytics\\outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953ce076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned dataset: 34,922 rows x 16 columns\n",
      "\n",
      "Columns: ['line_name', 'direction', 'stop_sequence', 'longitude', 'latitude', 'run_time_min', 'departure_hour', 'is_peak_hour', 'day_of_week_num', 'active_disruptions', 'unique_situations', 'avg_severity_score', 'planned_count', 'unplanned_count', 'roadworks_count', 'roadclosed_count']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_name</th>\n",
       "      <th>direction</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>run_time_min</th>\n",
       "      <th>departure_hour</th>\n",
       "      <th>is_peak_hour</th>\n",
       "      <th>day_of_week_num</th>\n",
       "      <th>active_disruptions</th>\n",
       "      <th>unique_situations</th>\n",
       "      <th>avg_severity_score</th>\n",
       "      <th>planned_count</th>\n",
       "      <th>unplanned_count</th>\n",
       "      <th>roadworks_count</th>\n",
       "      <th>roadclosed_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>51.485761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>1.609756</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288737</td>\n",
       "      <td>51.487744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>1.609756</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>outbound</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282997</td>\n",
       "      <td>51.490458</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>1.609756</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  line_name direction  stop_sequence  longitude   latitude  run_time_min  \\\n",
       "0        22  outbound              1   0.287746  51.485761           1.0   \n",
       "1        22  outbound              2   0.288737  51.487744           1.0   \n",
       "2        22  outbound              3   0.282997  51.490458           4.0   \n",
       "\n",
       "   departure_hour  is_peak_hour  day_of_week_num  active_disruptions  \\\n",
       "0            17.0             1                0                  82   \n",
       "1            17.0             1                0                  82   \n",
       "2            17.0             1                0                  82   \n",
       "\n",
       "   unique_situations  avg_severity_score  planned_count  unplanned_count  \\\n",
       "0                 68            1.609756             60               22   \n",
       "1                 68            1.609756             60               22   \n",
       "2                 68            1.609756             60               22   \n",
       "\n",
       "   roadworks_count  roadclosed_count  \n",
       "0               43                16  \n",
       "1               43                16  \n",
       "2               43                16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Cleaned Dataset\n",
    "# ============================================================\n",
    "\n",
    "pdf = pd.read_csv(os.path.join(DATA_DIR, 'cleaned_dataset.csv'))\n",
    "print(f\"Loaded cleaned dataset: {pdf.shape[0]:,} rows x {pdf.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(pdf.columns)}\")\n",
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544eb30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TARGET VARIABLE: high_disruption_risk\n",
      "============================================================\n",
      "\n",
      "active_disruptions distribution:\n",
      "count    34922.000000\n",
      "mean       111.791421\n",
      "std         40.022408\n",
      "min          3.000000\n",
      "25%         84.000000\n",
      "50%        107.000000\n",
      "75%        107.000000\n",
      "max        189.000000\n",
      "Name: active_disruptions, dtype: float64\n",
      "\n",
      "Percentiles:\n",
      "  25th percentile: 84\n",
      "  50th percentile: 107\n",
      "  60th percentile: 107\n",
      "  70th percentile: 107\n",
      "  75th percentile: 107\n",
      "  80th percentile: 173\n",
      "  90th percentile: 173\n",
      "\n",
      ">>> Chosen threshold: 107 (median)\n",
      "\n",
      "Target distribution:\n",
      "high_disruption_risk\n",
      "0    26774\n",
      "1     8148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance: 23.33% positive (high risk)\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "  + is_weekend (Sat/Sun flag)\n",
      "  + hour_sin, hour_cos (cyclical hour encoding)\n",
      "  + time_of_day (5 bins: ['night', 'morning_rush', 'midday', 'evening_rush', 'late_night'])\n",
      "  + lat_zone (4 latitude quartile zones)\n",
      "  + route_complexity (stop_sequence × run_time_min)\n",
      "\n",
      "Dropped 7 disruption columns (ALL leak target)\n",
      "Dataset now: 34,922 rows x 16 columns\n",
      "Columns: ['line_name', 'direction', 'stop_sequence', 'longitude', 'latitude', 'run_time_min', 'departure_hour', 'is_peak_hour', 'day_of_week_num', 'high_disruption_risk', 'is_weekend', 'hour_sin', 'hour_cos', 'time_of_day', 'lat_zone', 'route_complexity']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Create Target Variable & Engineer Features\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE: high_disruption_risk\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Understand disruption distribution\n",
    "print(\"\\nactive_disruptions distribution:\")\n",
    "print(pdf['active_disruptions'].describe())\n",
    "\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [25, 50, 60, 70, 75, 80, 90]:\n",
    "    val = pdf['active_disruptions'].quantile(p / 100)\n",
    "    print(f\"  {p}th percentile: {val:.0f}\")\n",
    "\n",
    "# Use MEDIAN as threshold -> balanced classes\n",
    "THRESHOLD = pdf['active_disruptions'].median()\n",
    "print(f\"\\n>>> Chosen threshold: {THRESHOLD:.0f} (median)\")\n",
    "\n",
    "# Create binary target\n",
    "pdf['high_disruption_risk'] = (pdf['active_disruptions'] > THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(pdf['high_disruption_risk'].value_counts())\n",
    "print(f\"\\nClass balance: {pdf['high_disruption_risk'].mean():.2%} positive (high risk)\")\n",
    "\n",
    "# ---- Feature Engineering (timetable-derived only) ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Weekend flag from day_of_week_num (5=Sat, 6=Sun)\n",
    "pdf['is_weekend'] = (pdf['day_of_week_num'] >= 5).astype(int)\n",
    "print(f\"  + is_weekend (Sat/Sun flag)\")\n",
    "\n",
    "# 2. Cyclical encoding of departure_hour (captures 23→0 wraparound)\n",
    "pdf['hour_sin'] = np.sin(2 * np.pi * pdf['departure_hour'] / 24)\n",
    "pdf['hour_cos'] = np.cos(2 * np.pi * pdf['departure_hour'] / 24)\n",
    "print(f\"  + hour_sin, hour_cos (cyclical hour encoding)\")\n",
    "\n",
    "# 3. Time-of-day bins (morning rush, midday, evening rush, night)\n",
    "bins = [0, 6, 10, 16, 20, 24]\n",
    "labels_td = ['night', 'morning_rush', 'midday', 'evening_rush', 'late_night']\n",
    "pdf['time_of_day'] = pd.cut(pdf['departure_hour'], bins=bins, labels=labels_td, right=False)\n",
    "pdf['time_of_day'] = pdf['time_of_day'].astype(str)\n",
    "print(f\"  + time_of_day (5 bins: {labels_td})\")\n",
    "\n",
    "# 4. Geographic zone (cluster stops by latitude bands)\n",
    "pdf['lat_zone'] = pd.qcut(pdf['latitude'], q=4, labels=['south', 'mid_south', 'mid_north', 'north'])\n",
    "pdf['lat_zone'] = pdf['lat_zone'].astype(str)\n",
    "print(f\"  + lat_zone (4 latitude quartile zones)\")\n",
    "\n",
    "# 5. Route complexity proxy (stops × run_time interaction)\n",
    "pdf['route_complexity'] = pdf['stop_sequence'] * pdf['run_time_min']\n",
    "print(f\"  + route_complexity (stop_sequence × run_time_min)\")\n",
    "\n",
    "# Drop ALL disruption-derived columns — they leak the target\n",
    "LEAKAGE_COLS = [\n",
    "    'active_disruptions',\n",
    "    'unique_situations',\n",
    "    'avg_severity_score',\n",
    "    'planned_count',\n",
    "    'unplanned_count',\n",
    "    'roadworks_count',\n",
    "    'roadclosed_count',\n",
    "]\n",
    "pdf = pdf.drop(columns=LEAKAGE_COLS)\n",
    "print(f\"\\nDropped {len(LEAKAGE_COLS)} disruption columns (ALL leak target)\")\n",
    "print(f\"Dataset now: {pdf.shape[0]:,} rows x {pdf.shape[1]} columns\")\n",
    "print(f\"Columns: {list(pdf.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603fd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/eda_plots.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_31372\\2007104469.py:53: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: EDA - Exploratory Visualizations\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Exploratory Data Analysis - Timetable Features vs Risk', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Target distribution\n",
    "pdf['high_disruption_risk'].value_counts().plot(\n",
    "    kind='bar', ax=axes[0, 0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0, 0].set_title('Target: High Disruption Risk')\n",
    "axes[0, 0].set_xticklabels(['Low Risk (0)', 'High Risk (1)'], rotation=0)\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Departure hour by risk\n",
    "for risk, color in [(0, '#2ecc71'), (1, '#e74c3c')]:\n",
    "    subset = pdf[pdf['high_disruption_risk'] == risk]\n",
    "    axes[0, 1].hist(subset['departure_hour'], bins=20, alpha=0.6,\n",
    "                    label=f'Risk={risk}', color=color)\n",
    "axes[0, 1].set_title('Departure Hour by Risk Level')\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Run time by risk\n",
    "pdf.boxplot(column='run_time_min', by='high_disruption_risk', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Run Time by Risk')\n",
    "axes[0, 2].set_xlabel('High Disruption Risk')\n",
    "plt.sca(axes[0, 2]); plt.title('Run Time (minutes) by Risk')\n",
    "\n",
    "# 4. Risk by time of day\n",
    "time_risk = pdf.groupby(['time_of_day', 'high_disruption_risk']).size().unstack(fill_value=0)\n",
    "time_risk.plot(kind='bar', ax=axes[1, 0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[1, 0].set_title('Risk by Time of Day')\n",
    "axes[1, 0].set_xlabel('Time Period')\n",
    "axes[1, 0].legend(['Low Risk', 'High Risk'])\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Risk by latitude zone\n",
    "lat_risk = pdf.groupby(['lat_zone', 'high_disruption_risk']).size().unstack(fill_value=0)\n",
    "lat_risk.plot(kind='bar', ax=axes[1, 1], color=['#2ecc71', '#e74c3c'])\n",
    "axes[1, 1].set_title('Risk by Geographic Zone')\n",
    "axes[1, 1].set_xlabel('Latitude Zone')\n",
    "axes[1, 1].legend(['Low Risk', 'High Risk'])\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Route complexity by risk\n",
    "pdf.boxplot(column='route_complexity', by='high_disruption_risk', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Route Complexity by Risk')\n",
    "plt.sca(axes[1, 2]); plt.title('Route Complexity by Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'eda_plots.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outputs/eda_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d5c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with target (high_disruption_risk):\n",
      "  is_weekend                +0.5159 █████████████████████████\n",
      "  run_time_min              +0.0422 ██\n",
      "  is_peak_hour              +0.0161 \n",
      "  hour_sin                  -0.0049 \n",
      "  departure_hour            -0.0096 \n",
      "  longitude                 -0.0166 \n",
      "  day_of_week_num           -0.0219 █\n",
      "  latitude                  -0.0348 █\n",
      "  route_complexity          -0.0390 █\n",
      "  hour_cos                  -0.0397 █\n",
      "  stop_sequence             -0.1074 █████\n",
      "\n",
      "Saved: outputs/correlation_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_31372\\3451154644.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Correlation Heatmap\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "corr = pdf[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0,\n",
    "            square=True, ax=ax, linewidths=0.5)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'correlation_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlations with target\n",
    "print(\"Correlation with target (high_disruption_risk):\")\n",
    "target_corr = corr['high_disruption_risk'].drop('high_disruption_risk').sort_values(ascending=False)\n",
    "for name, val in target_corr.items():\n",
    "    if pd.isna(val):\n",
    "        print(f\"  {name:25s}  NaN (constant or missing)\")\n",
    "        continue\n",
    "    bar = '█' * int(abs(val) * 50)\n",
    "    sign = '+' if val > 0 else '-'\n",
    "    print(f\"  {name:25s} {sign}{abs(val):.4f} {bar}\")\n",
    "\n",
    "print(\"\\nSaved: outputs/correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f8c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE SUMMARY\n",
      "==================================================\n",
      "Categorical features (4):\n",
      "  - line_name            (12 unique values)\n",
      "  - direction            (3 unique values)\n",
      "  - time_of_day          (5 unique values)\n",
      "  - lat_zone             (4 unique values)\n",
      "\n",
      "Numeric features (11):\n",
      "  - departure_hour\n",
      "  - is_peak_hour\n",
      "  - day_of_week_num\n",
      "  - stop_sequence\n",
      "  - latitude\n",
      "  - longitude\n",
      "  - run_time_min\n",
      "  - is_weekend\n",
      "  - hour_sin\n",
      "  - hour_cos\n",
      "  - route_complexity\n",
      "\n",
      "Target: high_disruption_risk\n",
      "Total base features: 15\n",
      "  (expands after one-hot encoding categoricals)\n",
      "\n",
      "NOTE: Zero disruption-derived features used — prevents leakage.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Define Feature & Target Columns\n",
    "# ============================================================\n",
    "\n",
    "# Categorical features (need encoding → one-hot)\n",
    "CAT_FEATURES = ['line_name', 'direction', 'time_of_day', 'lat_zone']\n",
    "\n",
    "# Numeric features (timetable + engineered)\n",
    "NUM_FEATURES = [\n",
    "    'departure_hour',\n",
    "    'is_peak_hour',\n",
    "    'day_of_week_num',\n",
    "    'stop_sequence',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'run_time_min',\n",
    "    'is_weekend',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'route_complexity',\n",
    "]\n",
    "\n",
    "TARGET = 'high_disruption_risk'\n",
    "\n",
    "ALL_FEATURES = CAT_FEATURES + NUM_FEATURES\n",
    "\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Categorical features ({len(CAT_FEATURES)}):\")\n",
    "for col in CAT_FEATURES:\n",
    "    print(f\"  - {col:20s} ({pdf[col].nunique()} unique values)\")\n",
    "print(f\"\\nNumeric features ({len(NUM_FEATURES)}):\")\n",
    "for col in NUM_FEATURES:\n",
    "    print(f\"  - {col}\")\n",
    "print(f\"\\nTarget: {TARGET}\")\n",
    "print(f\"Total base features: {len(ALL_FEATURES)}\")\n",
    "print(f\"  (expands after one-hot encoding categoricals)\")\n",
    "print(f\"\\nNOTE: Zero disruption-derived features used — prevents leakage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3466b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame: 34,922 rows\n",
      "root\n",
      " |-- line_name: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- time_of_day: string (nullable = true)\n",
      " |-- lat_zone: string (nullable = true)\n",
      " |-- departure_hour: double (nullable = true)\n",
      " |-- is_peak_hour: long (nullable = true)\n",
      " |-- day_of_week_num: long (nullable = true)\n",
      " |-- stop_sequence: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- run_time_min: double (nullable = true)\n",
      " |-- is_weekend: long (nullable = true)\n",
      " |-- hour_sin: double (nullable = true)\n",
      " |-- hour_cos: double (nullable = true)\n",
      " |-- route_complexity: double (nullable = true)\n",
      " |-- high_disruption_risk: long (nullable = true)\n",
      "\n",
      "\n",
      "Preprocessing pipeline:\n",
      "  1. StringIndexer\n",
      "  2. StringIndexer\n",
      "  3. StringIndexer\n",
      "  4. StringIndexer\n",
      "  5. OneHotEncoder\n",
      "  6. OneHotEncoder\n",
      "  7. OneHotEncoder\n",
      "  8. OneHotEncoder\n",
      "  9. VectorAssembler\n",
      "  10. StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Load into PySpark & Build Preprocessing Pipeline\n",
    "# ============================================================\n",
    "\n",
    "# Load pandas DF into Spark\n",
    "df = spark.createDataFrame(pdf[ALL_FEATURES + [TARGET]])\n",
    "print(f\"Spark DataFrame: {df.count():,} rows\")\n",
    "df.printSchema()\n",
    "\n",
    "# --- Build Pipeline ---\n",
    "# Step 1: StringIndexer for each categorical\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f'{col}_idx', handleInvalid='keep')\n",
    "    for col in CAT_FEATURES\n",
    "]\n",
    "\n",
    "# Step 2: OneHotEncoder for each indexed categorical\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f'{col}_idx', outputCol=f'{col}_vec')\n",
    "    for col in CAT_FEATURES\n",
    "]\n",
    "\n",
    "# Step 3: Assemble all features into a single vector\n",
    "encoded_cols = [f'{col}_vec' for col in CAT_FEATURES]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=NUM_FEATURES + encoded_cols,\n",
    "    outputCol='features_raw'\n",
    ")\n",
    "\n",
    "# Step 4: StandardScaler (important for Logistic Regression)\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features_raw', outputCol='features',\n",
    "    withStd=True, withMean=False\n",
    ")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "print(\"\\nPreprocessing pipeline:\")\n",
    "for i, stage in enumerate(preprocessing_pipeline.getStages(), 1):\n",
    "    print(f\"  {i}. {type(stage).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2e1bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 28,119 rows (80.5%)\n",
      "Test set:  6,803 rows (19.5%)\n",
      "\n",
      "Train class distribution:\n",
      "+--------------------+-----+\n",
      "|high_disruption_risk|count|\n",
      "+--------------------+-----+\n",
      "|                   0|21550|\n",
      "|                   1| 6569|\n",
      "+--------------------+-----+\n",
      "\n",
      "Test class distribution:\n",
      "+--------------------+-----+\n",
      "|high_disruption_risk|count|\n",
      "+--------------------+-----+\n",
      "|                   0| 5224|\n",
      "|                   1| 1579|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Train/Test Split (80/20)\n",
    "# ============================================================\n",
    "\n",
    "# Split BEFORE fitting pipeline (prevents data leakage)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train set: {train_df.count():,} rows ({train_df.count()/df.count()*100:.1f}%)\")\n",
    "print(f\"Test set:  {test_df.count():,} rows ({test_df.count()/df.count()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "train_df.groupBy(TARGET).count().orderBy(TARGET).show()\n",
    "\n",
    "print(f\"Test class distribution:\")\n",
    "test_df.groupBy(TARGET).count().orderBy(TARGET).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1b00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted on training data.\n",
      "Feature vector size: 35\n",
      "\n",
      "line_name encoding (12 categories):\n",
      "  22 -> 0\n",
      "  73 -> 1\n",
      "  33 -> 2\n",
      "  83 -> 3\n",
      "  x80 -> 4\n",
      "  44 -> 5\n",
      "  99 -> 6\n",
      "  88 -> 7\n",
      "  99OT -> 8\n",
      "  x32 -> 9\n",
      "  x2 -> 10\n",
      "  x1 -> 11\n",
      "\n",
      "direction encoding (3 categories):\n",
      "  outbound -> 0\n",
      "  inbound -> 1\n",
      "  clockwise -> 2\n",
      "\n",
      "time_of_day encoding (5 categories):\n",
      "  midday -> 0\n",
      "  morning_rush -> 1\n",
      "  evening_rush -> 2\n",
      "  late_night -> 3\n",
      "  night -> 4\n",
      "\n",
      "lat_zone encoding (4 categories):\n",
      "  mid_north -> 0\n",
      "  south -> 1\n",
      "  mid_south -> 2\n",
      "  north -> 3\n",
      "\n",
      "Sample prepared row:\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|features                                                                                                                                                                                                                                                                                     |high_disruption_risk|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "|(35,[0,1,3,4,5,6,8,9,10,11,24,28,32],[3.346209771296384,2.0549462920976227,0.1040064351044832,588.5574376053604,-0.8323468853091043,3.3648781672974004,-1.1655634469896103,-0.9401893423030238,0.5287610612267054,2.026636918441629,2.02694903878004,2.4327246236235593,2.2963086204612138]) |0                   |\n",
      "|(35,[0,1,3,4,5,6,8,9,10,11,24,28,32],[3.346209771296384,2.0549462920976227,0.1040064351044832,588.5574376053604,-0.8323468853091043,3.3648781672974004,-1.1655634469896103,-0.9401893423030238,0.5287610612267054,2.026636918441629,2.02694903878004,2.4327246236235593,2.2963086204612138]) |0                   |\n",
      "|(35,[0,1,3,4,5,6,8,9,10,11,24,28,32],[3.346209771296384,2.0549462920976227,0.31201930531344957,588.5254257445774,-1.0471103393973986,3.3648781672974004,-1.1655634469896103,-0.9401893423030238,1.5862831836801161,2.026636918441629,2.02694903878004,2.4327246236235593,2.2963086204612138])|0                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Fit Pipeline on Train, Transform Both\n",
    "# ============================================================\n",
    "\n",
    "# Fit on training data ONLY\n",
    "pipeline_model = preprocessing_pipeline.fit(train_df)\n",
    "\n",
    "# Transform both sets\n",
    "train_prepared = pipeline_model.transform(train_df)\n",
    "test_prepared = pipeline_model.transform(test_df)\n",
    "\n",
    "feature_size = train_prepared.select('features').first()[0].size\n",
    "print(f\"Pipeline fitted on training data.\")\n",
    "print(f\"Feature vector size: {feature_size}\")\n",
    "\n",
    "# Show encodings for each categorical\n",
    "n_indexers = len(CAT_FEATURES)\n",
    "for i, col in enumerate(CAT_FEATURES):\n",
    "    labels = pipeline_model.stages[i].labels\n",
    "    print(f\"\\n{col} encoding ({len(labels)} categories):\")\n",
    "    for j, label in enumerate(labels):\n",
    "        print(f\"  {label} -> {j}\")\n",
    "\n",
    "print(f\"\\nSample prepared row:\")\n",
    "train_prepared.select('features', TARGET).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/processed/featured_dataset.csv (34,922 x 16)\n",
      "Saved: data/processed/train_split.csv (28,119 rows)\n",
      "Saved: data/processed/test_split.csv (6,803 rows)\n",
      "Saved: data/processed/feature_metadata.json\n",
      "\n",
      "--- Feature Engineering Complete ---\n",
      "  Ready for 04_model_training.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Save Everything for Model Training\n",
    "# ============================================================\n",
    "\n",
    "# Save the featured pandas dataset (with target, before Spark encoding)\n",
    "pdf.to_csv(os.path.join(DATA_DIR, 'featured_dataset.csv'), index=False)\n",
    "print(f\"Saved: data/processed/featured_dataset.csv ({pdf.shape[0]:,} x {pdf.shape[1]})\")\n",
    "\n",
    "# Save train/test splits as CSV (avoids Hadoop/winutils issues on Windows)\n",
    "train_pdf = train_df.toPandas()\n",
    "test_pdf = test_df.toPandas()\n",
    "train_pdf.to_csv(os.path.join(DATA_DIR, 'train_split.csv'), index=False)\n",
    "test_pdf.to_csv(os.path.join(DATA_DIR, 'test_split.csv'), index=False)\n",
    "print(f\"Saved: data/processed/train_split.csv ({len(train_pdf):,} rows)\")\n",
    "print(f\"Saved: data/processed/test_split.csv ({len(test_pdf):,} rows)\")\n",
    "\n",
    "# Save metadata for next notebook — include all categorical label mappings\n",
    "import json\n",
    "\n",
    "cat_label_map = {}\n",
    "n_indexers = len(CAT_FEATURES)\n",
    "for i, col in enumerate(CAT_FEATURES):\n",
    "    cat_label_map[col] = list(pipeline_model.stages[i].labels)\n",
    "\n",
    "metadata = {\n",
    "    'target': TARGET,\n",
    "    'threshold': float(THRESHOLD),\n",
    "    'num_features': NUM_FEATURES,\n",
    "    'cat_features': CAT_FEATURES,\n",
    "    'feature_vector_size': feature_size,\n",
    "    'train_count': len(train_pdf),\n",
    "    'test_count': len(test_pdf),\n",
    "    'cat_labels': cat_label_map,\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, 'feature_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Saved: data/processed/models/feature_metadata.json\")\n",
    "\n",
    "print(f\"\\n--- Feature Engineering Complete ---\")\n",
    "print(f\"  Ready for 04_model_training.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee0c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
